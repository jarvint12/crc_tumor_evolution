{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d41e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import collections\n",
    "from math import log10, floor\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from process_input.input_to_2Dmatrix_transformer_classes import parse_matrices, FeatureDataset\n",
    "from model_skeletons.transformer.transformer_v15 import Transformer, model_version, WeightedMSELoss\n",
    "from compute_accuracy_transformer_classes import compute_accuracy\n",
    "from class_weights_cv import get_class_weights_transformer\n",
    "\n",
    "norm=True\n",
    "norm_type = \"max\"\n",
    "target_mode=\"whole_matrix\" #[\"whole_matrix\", \"whole_DNA_seq\", \"only_target_base\", \"target_with_landscape\"]\n",
    "if target_mode in [\"whole_matrix\", \"whole_DNA_seq\"]:\n",
    "    correct_label_index=16\n",
    "else:\n",
    "    correct_label_index=0\n",
    "model_script=os.path.join(\"model_skeletons\", \"transformer\", \"{}.py\".format(model_version)) #Used to read the file for logs\n",
    "assert os.path.isfile(model_script), \"Could not find file '{}'\".format(model_script)\n",
    "new_files_creation=\"create_new_data_files.py\" #Used to read the file for logs\n",
    "input_to_matrix=os.path.join(\"process_input\",\"input_to_2Dmatrix_transformer.py\") #Used to read the file for logs\n",
    "assert os.path.isfile(input_to_matrix), \"Could not find file '{}'\".format(input_to_matrix)\n",
    "compute_accuracy_file = \"compute_accuracy_transformer.py\"\n",
    "assert os.path.isfile(compute_accuracy_file), \"Could not find file '{}'\".format(compute_accuracy_file)\n",
    "valid_batch_size=5\n",
    "log_file=os.path.join(\"logs\", \"transformer\", \"{}.log\".format(model_version))\n",
    "cv_log_file=os.path.join(\"logs\", \"transformer\", \"cv_{}.log\".format(model_version))\n",
    "last_run_log_file=os.path.join(\"logs\", \"transformer\", \"{}_cv_all_runs.log\".format(model_version))\n",
    "last_run_log_file_final_runs=os.path.join(\"logs\", \"transformer\", \"{}_all_runs.log\".format(model_version))\n",
    "model_path=os.path.join(\"saved_models\", \"transformer\", \"{}.pth\".format(model_version))\n",
    "cv=False\n",
    "train=False\n",
    "indices_of_interest=[80,81,82,83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfa0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b60d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In input file, 33 base, 17th is altering, 1st A, 2nd C, 3rd G, 4th T\"\"\"\n",
    "class_types=['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG',\n",
    "            'GT', 'TA', 'TC', 'TG', 'TT']\n",
    "classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7}\n",
    "\n",
    "\n",
    "train_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_train.matrix\"\n",
    "valid_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_valid.matrix\"\n",
    "test_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_test.matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01149c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_input, trainset_target, trainset_mut_landscape, trainset_labels = parse_matrices(train_file, norm)\n",
    "valid_input, valid_target, valid_mut_landscape, valid_labels = parse_matrices(valid_file, norm)\n",
    "test_input, test_target, test_mut_landscape, test_labels = parse_matrices(test_file, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252a74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.tensor([0])\n",
    "eos_token = torch.tensor([5])\n",
    "\n",
    "trainset_input_with_tokens = torch.cat([trainset_input, eos_token.expand(trainset_input.size(0), 1)], dim=1)\n",
    "trainset_target_with_tokens = torch.cat([sos_token.expand(trainset_input.size(0), 1), trainset_target], dim=1)\n",
    "\n",
    "valid_input_with_tokens = torch.cat([valid_input, eos_token.expand(valid_input.size(0), 1)], dim=1)\n",
    "valid_target_with_tokens = torch.cat([sos_token.expand(valid_input.size(0), 1), valid_target], dim=1)\n",
    "\n",
    "test_input_with_tokens = torch.cat([test_input, eos_token.expand(test_input.size(0), 1)], dim=1)\n",
    "test_target_with_tokens = torch.cat([sos_token.expand(test_input.size(0), 1), test_target], dim=1)\n",
    "\n",
    "train_dataset=FeatureDataset(data=trainset_input_with_tokens, targets = trainset_target_with_tokens, mut_landscape =\n",
    "                             trainset_mut_landscape, labels = trainset_labels)\n",
    "valid_dataset=FeatureDataset(data=valid_input_with_tokens, targets=valid_target_with_tokens, mut_landscape =\n",
    "                            valid_mut_landscape, labels = valid_labels)\n",
    "test_dataset=FeatureDataset(data=test_input_with_tokens, targets=test_target_with_tokens, mut_landscape =\n",
    "                            test_mut_landscape, labels = test_labels)\n",
    "combined_valid_train = ConcatDataset([train_dataset, valid_dataset]) #Combines validation and training datasets for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd352e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=True\n",
    "    #,drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ecafa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3092, 7: 4301, 2: 127, 0: 476, 3: 641, 5: 303, 4: 181, 6: 123}\n"
     ]
    }
   ],
   "source": [
    "all_sequences = list()\n",
    "total_dup=0\n",
    "label_counts = dict()\n",
    "for sequences, targets, mut_landscape, labels in trainloader:\n",
    "    for i in range(sequences.shape[0]):\n",
    "        if not labels[i].item() in label_counts:\n",
    "            label_counts[labels[i].item()]=0\n",
    "        label_counts[labels[i].item()]+=1\n",
    "#         numbers = ','.join([str(x) for x in sequences[i,:,:,:].view(-1).tolist()])\n",
    "#         if not numbers in all_sequences:\n",
    "#             all_sequences.append(numbers)\n",
    "#         else:\n",
    "#             total_dup+=1\n",
    "            \n",
    "# print(total_dup)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6ff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log_file(learning_rate, optimizer_type, weight_decay, \n",
    "                      train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                      input_to_matrix, new_greatest_valid_acc, model_path, many_classes,\n",
    "                      compute_accuracy_file, model_script, run_name, dt_string, target_mode,\n",
    "                      nhead, num_encoder_layers, num_decoder_layers, norm, norm_type, acc_list=None):\n",
    "    \"\"\"Called, when new highest validation accuracy is found.\n",
    "    \n",
    "    Writes everything important information to a log file.\"\"\"\n",
    "    with open(log_file, 'w+') as fw:\n",
    "        fw.write(\"Created: {}\\nModel version: {}\\nPath: {}\\nRun name: {}\\nAccuracy: {}\\n\\n\".format(dt_string, model_version, \n",
    "                                                                                                   model_path, run_name, \n",
    "                                                                                                   new_greatest_valid_acc))\n",
    "        if acc_list:\n",
    "            for k, accuracy in enumerate(acc_list):\n",
    "                fw.write(\"Fold {}: {}.\\n\".format(k+1, accuracy))\n",
    "            fw.write('\\n')\n",
    "        fw.write(\"Hyperparameters:\\nOptimizer: {}\\n\".format(optimizer_type))\n",
    "        fw.write(\"Learning rate: {}\\nWeight decay: {}\\n\".format(learning_rate, weight_decay))\n",
    "        fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "        fw.write(\"Used MSELoss\\n\")\n",
    "        fw.write(\"Balanced classes\\n\")\n",
    "        fw.write(\"Data normalized: {}\\n\".format(norm))\n",
    "        fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "        fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "        fw.write(\"Train batch size: {}\\nValidation batch size: {}\\n\\n\".format(train_batch_size, valid_batch_size))\n",
    "        \n",
    "        fw.write(\"\\n--------------------------Script of the model can be seen below.---------------------------\\n\")\n",
    "        with open(model_script, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "    \n",
    "        fw.write(\"\\n------------------------------Created input matrices with script:------------------------------\\n\")\n",
    "        with open(input_to_matrix, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "        \n",
    "        fw.write(\"\\n------------------------------Computed accuracies with script:-----------------------------\\n\")\n",
    "        with open(compute_accuracy_file, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7deb54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In input file, 33 base, 17th is altering, 1st A, 2nd C, 3rd G, 4th T\"\"\"\n",
    "class_types=['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG',\n",
    "            'GT', 'TA', 'TC', 'TG', 'TT']\n",
    "classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e9cdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_train.matrix\"\n",
    "valid_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_valid.matrix\"\n",
    "test_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_test.matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fcb055",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_matrices() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainset_input, trainset_target, class_weights_train_whole \u001b[38;5;241m=\u001b[39m \u001b[43mparse_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m valid_input, valid_target, class_weights_valid_whole \u001b[38;5;241m=\u001b[39m parse_matrices(valid_file, norm, target_mode, norm_type)\n\u001b[1;32m      3\u001b[0m test_input, test_target, _ \u001b[38;5;241m=\u001b[39m parse_matrices(test_file, norm, target_mode, norm_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_matrices() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "trainset_input, trainset_target, class_weights_train_whole = parse_matrices(train_file, norm, target_mode, norm_type)\n",
    "valid_input, valid_target, class_weights_valid_whole = parse_matrices(valid_file, norm, target_mode, norm_type)\n",
    "test_input, test_target, _ = parse_matrices(test_file, norm, target_mode, norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.zeros((1, 1, trainset_input.shape[2]))\n",
    "eos_token = torch.ones((1, 1, trainset_input.shape[2]))\n",
    "\n",
    "trainset_input_with_tokens = torch.cat([trainset_input, eos_token.expand(trainset_input.size(0), 1, -1)], dim=1)\n",
    "trainset_target_with_tokens = torch.cat([sos_token.expand(trainset_input.size(0), 1, -1), trainset_target, eos_token.expand(trainset_input.size(0), 1, -1)], dim=1)\n",
    "\n",
    "valid_input_with_tokens = torch.cat([valid_input, eos_token.expand(valid_input.size(0), 1, -1)], dim=1)\n",
    "valid_target_with_tokens = torch.cat([sos_token.expand(valid_input.size(0), 1, -1), valid_target, eos_token.expand(valid_input.size(0), 1, -1)], dim=1)\n",
    "\n",
    "test_input_with_tokens = torch.cat([test_input, eos_token.expand(test_input.size(0), 1, -1)], dim=1)\n",
    "test_target_with_tokens = torch.cat([sos_token.expand(test_input.size(0), 1, -1), test_target, eos_token.expand(test_input.size(0), 1, -1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf868a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=FeatureDataset(data=trainset_input_with_tokens, labels=trainset_target_with_tokens)\n",
    "valid_dataset=FeatureDataset(data=valid_input_with_tokens, labels=valid_target_with_tokens)\n",
    "test_dataset=FeatureDataset(data=test_input_with_tokens, labels=test_target_with_tokens)\n",
    "combined_valid_train = ConcatDataset([train_dataset, valid_dataset]) #Combines validation and training datasets for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  correct_label_index, wandb, early_stop=100):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    min_tot_loss=float('inf')\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            labels_input = labels[:,:-1]\n",
    "            labels_expected = labels[:,1:]\n",
    "            \n",
    "            sequence_length = labels_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(sequence_length, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = net(sequences, labels_input, tgt_mask =tgt_mask)\n",
    "            #out = out.squeeze()\n",
    "            loss=criterion(sequences, labels_expected, out)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \n",
    "                                                \"VALID\", verbose = False, cv=True,\n",
    "                                                   correct_label_index=correct_label_index) #17th/33+Start token\n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "        if round(accuracy,3)<=round(greatest_acc,3):\n",
    "            pass\n",
    "        else:\n",
    "            greatest_acc=accuracy\n",
    "        if round(tot_valid_loss,3)>=round(min_tot_loss,3):\n",
    "            j+=1\n",
    "            if j>=early_stop and i>100:\n",
    "                break\n",
    "        else:\n",
    "            j=0\n",
    "            min_tot_loss=tot_valid_loss\n",
    "        \n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  correct_label_index, greatest_acc_overall, model_path, wandb, early_stop=50):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            labels_input = labels[:,:-1]\n",
    "            labels_expected = labels[:,1:]\n",
    "            \n",
    "            sequence_length = labels_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(sequence_length, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = net(sequences, labels_input, tgt_mask =tgt_mask)\n",
    "            #out=out.squeeze()\n",
    "            loss=criterion(sequences, labels_expected, out)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \"VALID\", \n",
    "                                                    verbose = False, cv = True, correct_label_index=correct_label_index) #17th/33+1\n",
    "        \n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "            \n",
    "        if round(accuracy,4)<=round(greatest_acc,4):\n",
    "            if early_stop:\n",
    "                j+=1\n",
    "                if j>=early_stop and i>100:\n",
    "                    print(\"Greates validation acc: {}\".format(greatest_acc))\n",
    "                    break\n",
    "        else:\n",
    "            if accuracy>greatest_acc_overall:\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "                greatest_acc_overall=accuracy\n",
    "            j=0\n",
    "            greatest_acc=accuracy\n",
    "    print(\"Greatest accuracy on run: {}\".format(greatest_acc))\n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394806c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earlier_accuracy(log_file):\n",
    "    with open(log_file, 'r') as fr:\n",
    "        for line in fr:\n",
    "            if \"Accuracy:\" in line:\n",
    "                return float(line.strip().split(' ')[1]) #Accuracy is written as Accuracy: <acc>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "greatest_avg_valid_acc = 0\n",
    "if os.path.isfile(cv_log_file):\n",
    "    greatest_avg_valid_acc = get_earlier_accuracy(cv_log_file)\n",
    "print(greatest_avg_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e95b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(last_run_log_file):\n",
    "    with open(last_run_log_file, 'w+') as fw:\n",
    "        fw.write(\"Run log.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(device, dataloader, indices_of_interest, norm_type):\n",
    "    total_amount=0\n",
    "    class_amounts=collections.Counter()\n",
    "    bases = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    for sequences, labels in dataloader:\n",
    "        values_of_interest = sequences[:, 16, indices_of_interest]\n",
    "        _, original_bases = torch.max(values_of_interest, dim=1)\n",
    "\n",
    "        values_of_interest = labels[:, correct_label_index, indices_of_interest] #\n",
    "        _, new_bases = torch.max(values_of_interest, dim=1)\n",
    "        for new_base, original_base in zip(new_bases, original_bases):\n",
    "            correct_class = classes[bases[original_base.item()]+bases[new_base.item()]]\n",
    "            class_amounts[correct_class]+=1\n",
    "            total_amount+=1\n",
    "            \n",
    "    return create_class_weights(class_amounts, total_amount, norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv:\n",
    "    k_folds=5\n",
    "    epochs=5000\n",
    "    seq_len=33\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True) #batch size affects the size of datasets\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    for i in range(150): #Test with 150 different hyperparameter combinations\n",
    "        valid_accuracies = list()\n",
    "        learning_rate=random.sample([0.0001, 0.00001, 0.000001], 1)[0]\n",
    "        lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "        train_batch_size=random.sample([32, 64, 128, 256], 1)[0]\n",
    "        norm_type = random.sample([\"sum\", \"max\", \"None\"], 1)[0]\n",
    "        \n",
    "        nhead=random.sample([2, 3, 4, 6, 7, 12, 21, 42], 1)[0]\n",
    "        num_encoder_layers=random.sample([2, 3, 4, 6, 8], 1)[0]\n",
    "        num_decoder_layers=num_encoder_layers\n",
    "        \n",
    "        optimizer_type=random.sample([\"Adam\",\"AdamW\"], 1)[0] #random.sample([\"Adam\",\"SGD\"], 1)[0]\n",
    "        weight_decay=random.sample([0, 0.000001, 0.00000001], 1)[0]\n",
    "        if weight_decay!=0: weight_decay = round(weight_decay, -int(floor(log10(weight_decay))) + 2)\n",
    "        decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "        for (train_ids, test_ids) in kfold.split(combined_valid_train):\n",
    "            \n",
    "            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                          combined_valid_train, \n",
    "                          batch_size=train_batch_size, sampler=train_subsampler)\n",
    "            validloader = torch.utils.data.DataLoader(\n",
    "                              combined_valid_train,\n",
    "                              batch_size=valid_batch_size, sampler=test_subsampler)\n",
    "            train_class_weights = get_class_weights(device, trainloader, indices_of_interest, norm_type)\n",
    "            valid_class_weights = get_class_weights(device, validloader, indices_of_interest, norm_type)\n",
    "            \n",
    "            net = Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers).to(device)\n",
    "            if optimizer_type==\"Adam\":\n",
    "                optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            elif optimizer_type==\"AdamW\":\n",
    "                optimizer=torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            else:\n",
    "                raise RuntimeError(\"WRONG OPTIMIZER: {}\".format(optimizer_type))\n",
    "            criterion = WeightedMSELoss(device, train_class_weights, classes, correct_label_index, indices_of_interest)\n",
    "            valid_criterion=WeightedMSELoss(device, valid_class_weights, classes, correct_label_index, indices_of_interest)\n",
    "            run_name = None\n",
    "            valid_acc = cv_train_network(net,criterion,valid_criterion,epochs,optimizer,trainloader, validloader, \n",
    "                                      correct_label_index, wandb=None)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "        avg_valid_acc = sum(valid_accuracies) / len(valid_accuracies)\n",
    "\n",
    "        print(\"Optimizer: {}\\nLearning Rate: {}\".format(optimizer_type, learning_rate))\n",
    "        print(\"Weight decay: {}\".format(weight_decay))\n",
    "        print(\"Average validation accuracy: {}\".format(avg_valid_acc))\n",
    "        for k, accuracy in enumerate(valid_accuracies):\n",
    "            print(\"Fold {}: {}.\".format(k+1, accuracy))\n",
    "\n",
    "        with open(last_run_log_file, 'a+') as fw:\n",
    "            fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "            fw.write(\"Average validation accuracy: {}\\n\".format(avg_valid_acc))\n",
    "            fw.write(\"\\n\".join([\"Fold {}: {}.\".format(k+1, accuracy) for k, accuracy in enumerate(valid_accuracies)]))\n",
    "            fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\n\".format(optimizer_type, learning_rate))\n",
    "            fw.write(\"Weight decay: {}\\nBatch size: {}\\n\".format(weight_decay, train_batch_size))\n",
    "            fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "            fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "            fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "            fw.write('\\n\\n')\n",
    "\n",
    "        if avg_valid_acc>greatest_avg_valid_acc:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay,\n",
    "                             train_batch_size, valid_batch_size, cv_log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              avg_valid_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string, target_mode, \n",
    "                              nhead, num_encoder_layers, num_decoder_layers, acc_list=valid_accuracies, norm=norm,\n",
    "                                 norm_type=norm_type)\n",
    "            greatest_avg_valid_acc=avg_valid_acc\n",
    "            os.path.join(\"saved_models\", \"transformer\", \"test_notebook.pth\")\n",
    "            torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a20f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "greatest_acc_overall=0\n",
    "if os.path.isfile(log_file):\n",
    "    greatest_acc_overall=get_earlier_accuracy(log_file)\n",
    "print(greatest_acc_overall)\n",
    "train_batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset\n",
    "    ,batch_size=train_batch_size\n",
    "    ,shuffle=True\n",
    "    ,drop_last=True\n",
    ")\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    learning_rate=0.000001\n",
    "    optimizer_type=\"AdamW\"\n",
    "    weight_decay=0.00000001\n",
    "    nhead=3\n",
    "    num_encoder_layers=2\n",
    "    num_decoder_layers=2\n",
    "    epochs=5000\n",
    "    norm_type=\"None\"\n",
    "    criterion = WeightedMSELoss(device, class_weights_train_whole, classes, correct_label_index, indices_of_interest)\n",
    "    valid_criterion=WeightedMSELoss(device, class_weights_valid_whole, classes, correct_label_index, indices_of_interest)\n",
    "    lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "    decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "    for i in range(1000):\n",
    "        net = Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers).to(device)\n",
    "        if optimizer_type==\"Adam\":\n",
    "            optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type==\"AdamW\":\n",
    "            optimizer=torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise RuntimeError(\"WRONG OPTIMIZER.\")\n",
    "        run_name=None\n",
    "        greatest_acc = train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                          correct_label_index, greatest_acc_overall, model_path, wandb=None, early_stop=None)\n",
    "        \n",
    "        \n",
    "        print(\"Accuracy: {}\".format(greatest_acc))\n",
    "        \n",
    "        with open(last_run_log_file_final_runs, 'a+') as fw:\n",
    "            fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "            fw.write(\"Validation accuracy: {}\\n\".format(greatest_acc))\n",
    "            fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\n\".format(optimizer_type, learning_rate))\n",
    "            fw.write(\"Weight decay: {}\\nBatch size: {}\\n\".format(weight_decay, train_batch_size))\n",
    "            fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "            fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "            fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "            fw.write('\\n\\n')\n",
    "       \n",
    "        \n",
    "        if greatest_acc>greatest_acc_overall:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay,\n",
    "                             train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              greatest_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string, target_mode, \n",
    "                              nhead, num_encoder_layers, num_decoder_layers, norm=norm,\n",
    "                                 norm_type=norm_type)\n",
    "            greatest_acc_overall=greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Transformer(nhead=42,num_encoder_layers=3,num_decoder_layers=3)\n",
    "model_path_download = os.path.join(\"saved_models\", \"transformer\", \"test.pth\")\n",
    "model.load_state_dict(torch.load(model_path_download))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07135493",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(test_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86445702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models/transformer/transformer_v15.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(6, 84)\n",
       "  (positional_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=84, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=84, bias=True)\n",
       "          (norm1): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=84, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=84, bias=True)\n",
       "          (norm1): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=2876, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_path=os.path.join(\"saved_models\", \"cnn\", \"cnn_v1_11_backup.pth\".format(model_version))\n",
    "#model_path=os.path.join(\"saved_models\", \"cnn\", \"cnn_v1_15.pth\".format(model_version))\n",
    "#model = model_skeleton(many_classes)\n",
    "nhead=6\n",
    "num_encoder_layers=4\n",
    "num_decoder_layers=4\n",
    "model=Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print(model_path)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f57d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/77/jarvint12/unix/anaconda3/envs/jupyter2/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CA: Correct 13, total 16, acc 0.8125\n",
      "Class CC: Correct 27, total 42, acc 0.6428571428571429\n",
      "Class CG: Correct 5, total 9, acc 0.5555555555555556\n",
      "Class CT: Correct 6, total 19, acc 0.3157894736842105\n",
      "Class TA: Correct 8, total 14, acc 0.5714285714285714\n",
      "Class TC: Correct 11, total 13, acc 0.8461538461538461\n",
      "Class TG: Correct 18, total 27, acc 0.6666666666666666\n",
      "Class TT: Correct 46, total 56, acc 0.8214285714285714\n",
      "\n",
      " TEST\n",
      "TP: 61 . FN: 37 TP/(TP+FN): 0.6224489795918368 TN: 73 FP: 25 TN/(TN+FP): 0.7448979591836735 Wrong positive class predicted: 22 Wrong negative class predicted: 0\n",
      "Fake F1-score: 0.8058252427184466 . Fake F2-score: 0.8299999999999998\n",
      "Fake TP/(TP+FN): 0.6916666666666667 Fake TN/(TN+FP) 0.7448979591836735\n",
      "Fake precision: 0.7685185185185185 Fake recall: 0.8469387755102041\n",
      "F1-score: 0.6630434782608696\n",
      "F2-score: 0.6380753138075315\n",
      "Precision: 0.7093023255813954\n",
      "Recall: 0.6224489795918368\n",
      "Fake accuracy: 0.7959183673469388\n",
      "0.6540474784718205\n"
     ]
    }
   ],
   "source": [
    "accuracy, loss = \\\n",
    "compute_accuracy(device, model, testloader, None, \n",
    "                                                \"TEST\", verbose = True, cv=False,\n",
    "                                                   correct_label_index=correct_label_index)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f1bef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CA: Correct 63, total 84, acc 0.75\n",
      "Class CC: Correct 384, total 545, acc 0.7045871559633028\n",
      "Class CG: Correct 14, total 22, acc 0.6363636363636364\n",
      "Class CT: Correct 65, total 112, acc 0.5803571428571429\n",
      "Class TA: Correct 13, total 31, acc 0.41935483870967744\n",
      "Class TC: Correct 39, total 53, acc 0.7358490566037735\n",
      "Class TG: Correct 11, total 21, acc 0.5238095238095238\n",
      "Class TT: Correct 588, total 758, acc 0.7757255936675461\n",
      "\n",
      " VALID\n",
      "TP: 205 . FN: 118 TP/(TP+FN): 0.6346749226006192 TN: 972 FP: 331 TN/(TN+FP): 0.7459708365310821 Wrong positive class predicted: 57 Wrong negative class predicted: 0\n",
      "Fake F1-score: 0.572052401746725 . Fake F2-score: 0.6949602122015915\n",
      "Fake TP/(TP+FN): 0.6894736842105263 Fake TN/(TN+FP) 0.7459708365310821\n",
      "Fake precision: 0.4418212478920742 Fake recall: 0.8111455108359134\n",
      "F1-score: 0.47729918509895236\n",
      "F2-score: 0.5607221006564552\n",
      "Precision: 0.3824626865671642\n",
      "Recall: 0.6346749226006192\n",
      "Fake accuracy: 0.7589175891758918\n",
      "0.6407558684968253\n"
     ]
    }
   ],
   "source": [
    "validloader = torch.utils.data.DataLoader(valid_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")\n",
    "\n",
    "accuracy, loss = \\\n",
    "compute_accuracy(device, model, validloader, None, \n",
    "                                                \"VALID\", verbose = True, cv=False,\n",
    "                                                   correct_label_index=correct_label_index)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74565c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CA: Correct 386, total 476, acc 0.8109243697478992\n",
      "Class CC: Correct 2239, total 3092, acc 0.7241267787839586\n",
      "Class CG: Correct 102, total 127, acc 0.8031496062992126\n",
      "Class CT: Correct 389, total 641, acc 0.6068642745709828\n",
      "Class TA: Correct 129, total 181, acc 0.712707182320442\n",
      "Class TC: Correct 247, total 303, acc 0.8151815181518152\n",
      "Class TG: Correct 110, total 123, acc 0.8943089430894309\n",
      "Class TT: Correct 3268, total 4301, acc 0.7598232969076959\n",
      "\n",
      " TRAIN\n",
      "TP: 1363 . FN: 488 TP/(TP+FN): 0.7363587250135062 TN: 5507 FP: 1886 TN/(TN+FP): 0.7448938184769377 Wrong positive class predicted: 180 Wrong negative class predicted: 0\n",
      "Fake F1-score: 0.584469696969697 . Fake F2-score: 0.7121757592541309\n",
      "Fake TP/(TP+FN): 0.7597242737567701 Fake TN/(TN+FP) 0.7448938184769377\n",
      "Fake precision: 0.44998541848935547 Fake recall: 0.8336034575904916\n",
      "F1-score: 0.5345098039215685\n",
      "F2-score: 0.6397258988078475\n",
      "Precision: 0.41951369652200676\n",
      "Recall: 0.7363587250135062\n",
      "Fake accuracy: 0.7626568585028126\n",
      "0.7658857462339296\n"
     ]
    }
   ],
   "source": [
    "validloader = torch.utils.data.DataLoader(train_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")\n",
    "\n",
    "accuracy, loss = \\\n",
    "compute_accuracy(device, model, validloader, None, \n",
    "                                                \"TRAIN\", verbose = True, cv=False,\n",
    "                                                   correct_label_index=correct_label_index)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
