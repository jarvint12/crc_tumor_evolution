{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c1621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import wandb\n",
    "\n",
    "from math import log10, floor\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from process_input.input_to_1D_feature_matrix import parse_matrices, FeatureDataset\n",
    "from model_skeletons.fc_basemodel.base_fc_v3 import base_fc, model_version\n",
    "from compute_accuracy import compute_accuracy\n",
    "from class_weights_cv import get_class_weights\n",
    "\n",
    "model_script=os.path.join(\"model_skeletons\", \"fc_basemodel\", \"{}.py\".format(model_version)) #Used to read the file for logs\n",
    "assert os.path.isfile(model_script), \"Could not find file '{}'\".format(model_script)\n",
    "new_files_creation=\"create_new_data_files.py\" #Used to read the file for logs\n",
    "input_to_matrix=os.path.join(\"process_input\", \"input_to_1D_feature_matrix.py\") #Used to read the file for logs\n",
    "assert os.path.isfile(input_to_matrix), \"Could not find file '{}'\".format(input_to_matrix)\n",
    "compute_accuracy_file = \"compute_accuracy.py\"\n",
    "assert os.path.isfile(compute_accuracy_file), \"Could not find file '{}'\".format(compute_accuracy_file)\n",
    "valid_batch_size=5\n",
    "log_file=os.path.join(\"logs\", \"basemodel_fc\", \"{}.log\".format(model_version))\n",
    "cv_log_file=os.path.join(\"logs\", \"basemodel_fc\", \"cv_{}.log\".format(model_version))\n",
    "last_run_log_file=os.path.join(\"logs\", \"basemodel_fc\", \"current_run_log_{}.log\".format(model_version))\n",
    "last_run_log_file_final_runs=os.path.join(\"logs\", \"basemodel_fc\", \"{}_all_runs.log\".format(model_version))\n",
    "model_path=os.path.join(\"saved_models\", \"basemodel_fc\", \"{}.pth\".format(model_version))\n",
    "cv=False\n",
    "train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda')\n",
    "many_classes=True #ACGT order in onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44222d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log_file(learning_rate, optimizer_type, weight_decay, momentum, is_scheduler, T_max, smoothing, \n",
    "                      train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                      input_to_matrix, new_greatest_valid_acc, model_path, many_classes,\n",
    "                      compute_accuracy_file, model_script, run_name, dt_string, acc_list=None):\n",
    "    \"\"\"Called, when new highest validation accuracy is found.\n",
    "    \n",
    "    Writes everything important information to a log file.\"\"\"\n",
    "    with open(log_file, 'w+') as fw:\n",
    "        fw.write(\"Created: {}\\nModel version: {}\\nPath: {}\\nRun name: {}\\nAccuracy: {}\\n\\n\".format(dt_string, model_version, \n",
    "                                                                                                   model_path, run_name, \n",
    "                                                                                                   new_greatest_valid_acc))\n",
    "        if acc_list:\n",
    "            for k, accuracy in enumerate(acc_list):\n",
    "                fw.write(\"Fold {}: {}.\\n\".format(k+1, accuracy))\n",
    "            fw.write('\\n')\n",
    "        fw.write(\"Hyperparameters:\\nOptimizer: {}\\n\".format(optimizer_type))\n",
    "        if optimizer_type==\"SGD\":\n",
    "            fw.write(\"Momentum: {}\\n\".format(momentum))\n",
    "        fw.write(\"Learning rate: {}\\nWeight decay: {}\\n\".format(learning_rate, weight_decay))\n",
    "        if is_scheduler:\n",
    "            fw.write(\"Used CosineAnnealingLR scheduler with T_max {}\\n\".format(T_max))\n",
    "        if many_classes:\n",
    "            fw.write(\"Used CrossEntropyLoss with label smoothing {}\\n\".format(smoothing))\n",
    "            fw.write(\"Balanced classes\\n\")\n",
    "        else:\n",
    "            fw.write(\"Used BCELoss without label smoothing\\n\")\n",
    "        fw.write(\"Train batch size: {}\\nValidation batch size: {}\\n\\n\".format(train_batch_size, valid_batch_size))\n",
    "        \n",
    "        fw.write(\"\\n--------------------------Script of the model can be seen below.---------------------------\\n\")\n",
    "        with open(model_script, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "    \n",
    "        fw.write(\"\\n------------------------------Created input matrices with script:------------------------------\\n\")\n",
    "        with open(input_to_matrix, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "        \n",
    "        fw.write(\"\\n------------------------------Computed accuracies with script:-----------------------------\\n\")\n",
    "        with open(compute_accuracy_file, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e654cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_types=['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG',\n",
    "            'GT', 'TA', 'TC', 'TG', 'TT']\n",
    "classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bc2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_train.matrix\"\n",
    "valid_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_valid.matrix\"\n",
    "test_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_test.matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1153f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = parse_matrices(train_file, classes)\n",
    "valid_input, valid_target = parse_matrices(valid_file, classes)\n",
    "test_input, test_target = parse_matrices(test_file, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=FeatureDataset(data=train_input, labels=train_target)\n",
    "valid_dataset=FeatureDataset(data=valid_input, labels=valid_target)\n",
    "test_dataset=FeatureDataset(data=test_input, labels=test_target)\n",
    "combined_valid_train = ConcatDataset([train_dataset, valid_dataset]) #Combines validation and training datasets for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9750ad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9244])\n",
      "tensor(6152) tensor(4943)\n",
      "Train neg, pos: tensor(1851) tensor(7393)\n",
      "Validation neg, pos: tensor(323) tensor(1303)\n",
      "Test neg, pos: tensor(98) tensor(98)\n"
     ]
    }
   ],
   "source": [
    "print(train_target.shape)\n",
    "print(torch.sum(train_target!=1),torch.sum(train_target!=7))\n",
    "print(\"Train neg, pos:\",train_target.shape[0]-torch.sum(train_target==1)-torch.sum(train_target==7),\n",
    "      torch.sum(train_target==1)+torch.sum(train_target==7))\n",
    "print(\"Validation neg, pos:\",valid_target.shape[0]-torch.sum(valid_target==1)-torch.sum(valid_target==7),\n",
    "      torch.sum(valid_target==1)+torch.sum(valid_target==7))\n",
    "print(\"Test neg, pos:\",test_target.shape[0]-torch.sum(test_target==1)-torch.sum(test_target==7),\n",
    "      torch.sum(test_target==1)+torch.sum(test_target==7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd6f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  is_scheduler,scheduler, wandb, early_stop=100):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    min_tot_loss=float('inf')\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out=net(sequences)\n",
    "            out=out.squeeze()\n",
    "            loss=criterion(out,labels)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        if is_scheduler:\n",
    "            scheduler.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \n",
    "                                                \"VALID\", verbose = False, cv=True)\n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "        if round(accuracy,3)<=round(greatest_acc,3):\n",
    "            pass\n",
    "        else:\n",
    "            greatest_acc=accuracy\n",
    "        if round(tot_valid_loss,3)>=round(min_tot_loss,3):\n",
    "            j+=1\n",
    "            if j>=early_stop and i>100:\n",
    "                break\n",
    "        else:\n",
    "            j=0\n",
    "            min_tot_loss=tot_valid_loss\n",
    "        \n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b35ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  is_scheduler,scheduler, greatest_acc_overall, model_path, wandb, early_stop=50):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out=net(sequences)\n",
    "            out=out.squeeze()\n",
    "            loss=criterion(out,labels)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        if is_scheduler:\n",
    "            scheduler.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \"VALID\", verbose = False, cv = True)\n",
    "        \n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "            \n",
    "        if round(accuracy,3)<=round(greatest_acc,3):\n",
    "            if early_stop:\n",
    "                j+=1\n",
    "                if j>=early_stop and i>100:\n",
    "                    print(\"Greates validation acc: {}\".format(greatest_acc))\n",
    "                    break\n",
    "        else:\n",
    "            if accuracy>greatest_acc_overall:\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "                greatest_acc_overall=accuracy\n",
    "            j=0\n",
    "            greatest_acc=accuracy\n",
    "    print(\"Greatest accuracy on run: {}\".format(greatest_acc))\n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814aeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earlier_accuracy(log_file):\n",
    "    with open(log_file, 'r') as fr:\n",
    "        for line in fr:\n",
    "            if \"Accuracy:\" in line:\n",
    "                return float(line.strip().split(' ')[1]) #Accuracy is written as Accuracy: <acc>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1256a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2627335095525115\n"
     ]
    }
   ],
   "source": [
    "greatest_avg_valid_acc = 0\n",
    "if os.path.isfile(cv_log_file):\n",
    "    greatest_avg_valid_acc = get_earlier_accuracy(cv_log_file)\n",
    "print(greatest_avg_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0ddea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(last_run_log_file):\n",
    "    with open(last_run_log_file, 'w+') as fw:\n",
    "        fw.write(\"Run log.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a2db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_fold cross_validation for hyperparameters\n",
    "if cv:\n",
    "    k_folds = 5\n",
    "    epochs=5000\n",
    "    run_name=\"None\"\n",
    "    testloader=None\n",
    "\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True) #batch size affects the size of datasets\n",
    "    for i in range(150): #Test with 30 different hyperparameter combinations\n",
    "        valid_accuracies = list()\n",
    "        mom_text=\"\"\n",
    "        scheduler_text=\"\"\n",
    "        momentum=None\n",
    "        T_max=None\n",
    "        is_scheduler=False\n",
    "        scheduler=None\n",
    "\n",
    "        learning_rate=random.sample([0.01, 0.001, 0.0001, 0.00001], 1)[0]\n",
    "        lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "        train_batch_size=random.sample([32, 64, 128], 1)[0]\n",
    "\n",
    "        optimizer_type=random.sample([\"Adam\",\"SGD\"], 1)[0]\n",
    "        if optimizer_type==\"SGD\":\n",
    "            momentum= random.sample([0, np.random.uniform()], 1)[0]\n",
    "            if momentum!=0: momentum = round(momentum, -int(floor(log10(momentum))) + 2)\n",
    "            mom_text=\"_mom\"+str(round(momentum,2)).replace(\".\",\"d\")\n",
    "            is_scheduler=random.sample([True, False], 1)[0]\n",
    "            if is_scheduler:\n",
    "                T_max=random.sample([1, np.random.uniform(low=0.2)], 1)[0]\n",
    "                T_max = round(T_max, -int(floor(log10(T_max))) + 2)\n",
    "        weight_decay=random.sample([0, 0.00001, 0.0001, 0.001], 1)[0]\n",
    "        if weight_decay!=0: weight_decay = round(weight_decay, -int(floor(log10(weight_decay))) + 2)\n",
    "        decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "        smoothing=random.sample([0, np.random.uniform(high=0.05)], 1)[0]\n",
    "        if smoothing!=0: smoothing = round(smoothing, -int(floor(log10(smoothing))) + 2)\n",
    "\n",
    "        for (train_ids, test_ids) in kfold.split(combined_valid_train):\n",
    "            # Sample elements randomly from a given list of ids, no replacement.\n",
    "            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                          combined_valid_train, \n",
    "                          batch_size=train_batch_size, sampler=train_subsampler)\n",
    "            validloader = torch.utils.data.DataLoader(\n",
    "                              combined_valid_train,\n",
    "                              batch_size=valid_batch_size, sampler=test_subsampler)\n",
    "            net = base_fc().to(device)\n",
    "            if optimizer_type==\"Adam\":\n",
    "                optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            elif optimizer_type==\"SGD\":\n",
    "                optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, \n",
    "                                            momentum=momentum, weight_decay=weight_decay)\n",
    "                if is_scheduler:\n",
    "                    scheduler=lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max*epochs)\n",
    "            else:\n",
    "                raise RuntimeError(\"WRONG OPTIMIZER: {}\".format(optimizer_type))\n",
    "\n",
    "\n",
    "            train_class_weights, valid_class_weights = get_class_weights(device, trainloader, validloader)\n",
    "            criterion=nn.CrossEntropyLoss(weight=train_class_weights, label_smoothing=smoothing)\n",
    "            valid_criterion=nn.CrossEntropyLoss(weight=valid_class_weights)\n",
    "            \n",
    "            os.system(\"rm -rf masters_thesis/base_model_fc/wandb/run-*\")\n",
    "            run = wandb.init(project='MLP')\n",
    "            run_name = \"batch\"+str(train_batch_size)+\"_lr-\"+lr_text \\\n",
    "            +\"smooth\"+str(round(smoothing,2)).replace('.','d')+\"optim-\"+optimizer_type+decay_text \\\n",
    "            +\"_sch-\"+str(is_scheduler)+scheduler_text+mom_text\n",
    "            wandb.run.name = run_name\n",
    "            config = wandb.config\n",
    "            config.batch_size=train_batch_size\n",
    "            #config.optimizer_type=optimizer_type\n",
    "            #config.learning_rate = learning_rate\n",
    "            #config.weight_decay=weight_decay\n",
    "            #config.is_scheduler=is_scheduler\n",
    "\n",
    "            valid_acc = cv_train_network(net,criterion,valid_criterion,epochs,optimizer,trainloader, validloader, \n",
    "                                      is_scheduler,scheduler, wandb)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "        avg_valid_acc = sum(valid_accuracies) / len(valid_accuracies)\n",
    "\n",
    "        print(\"Optimizer: {}\\nLearning Rate: {}\\nScheduler: {}\".format(optimizer_type, learning_rate, is_scheduler))\n",
    "        print(\"Weight decay: {}\\nSmoothing: {}\".format(weight_decay, smoothing))\n",
    "        if optimizer_type==\"SGD\":\n",
    "            print(\"Momentum: {}\".format(momentum))\n",
    "            if is_scheduler:\n",
    "                print(\"T_max: {}\\n\".format(T_max))\n",
    "        print(\"Average validation accuracy: {}\".format(avg_valid_acc))\n",
    "        for k, accuracy in enumerate(valid_accuracies):\n",
    "            print(\"Fold {}: {}.\".format(k+1, accuracy))\n",
    "\n",
    "        with open(last_run_log_file, 'a+') as fw:\n",
    "            fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "            fw.write(\"Average validation accuracy: {}\\n\".format(avg_valid_acc))\n",
    "            fw.write(\"\\n\".join([\"Fold {}: {}.\".format(k+1, accuracy) for k, accuracy in enumerate(valid_accuracies)]))\n",
    "            fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\nScheduler: {}\\n\".format(optimizer_type, learning_rate, is_scheduler))\n",
    "            fw.write(\"Weight decay: {}\\nSmoothing: {}\\nBatch size: {}\\n\".format(weight_decay, smoothing, train_batch_size))\n",
    "            if optimizer_type==\"SGD\":\n",
    "                fw.write(\"Momentum: {}\\n\".format(momentum))\n",
    "                if is_scheduler:\n",
    "                    fw.write(\"T_max: {}\\n\".format(T_max))\n",
    "            fw.write('\\n\\n')\n",
    "\n",
    "        if avg_valid_acc>greatest_avg_valid_acc:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay, momentum, is_scheduler, T_max,\n",
    "                             smoothing, train_batch_size, valid_batch_size, cv_log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              avg_valid_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string, acc_list=valid_accuracies)\n",
    "            greatest_avg_valid_acc=avg_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e3587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2596905056830896\n"
     ]
    }
   ],
   "source": [
    "greatest_acc_overall=0\n",
    "if os.path.isfile(log_file):\n",
    "    greatest_acc_overall=get_earlier_accuracy(log_file)\n",
    "print(greatest_acc_overall)\n",
    "train_batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36387220",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset\n",
    "    ,batch_size=train_batch_size\n",
    "    ,shuffle=True\n",
    "    ,drop_last=True\n",
    ")\n",
    "validloader = DataLoader(valid_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ccd9f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4275, 0.3737, 9.0984, 1.8027, 6.3840, 3.8135, 9.3943, 0.2687],\n",
      "       device='cuda:0') tensor([2.4275, 0.3737, 9.0984, 1.8027, 6.3840, 3.8135, 9.3943, 0.2687],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if many_classes:\n",
    "    class_weights=class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_target),\n",
    "        y=np.array(train_target))\n",
    "    class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "    class_weights=class_weights.to(device)\n",
    "    \n",
    "    valid_class_weights=class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_target),\n",
    "        y=np.array(train_target))\n",
    "    valid_class_weights=torch.tensor(valid_class_weights,dtype=torch.float)\n",
    "    valid_class_weights=valid_class_weights.to(device)\n",
    "    \n",
    "else:\n",
    "    print(np.array(train_target))\n",
    "print(class_weights, valid_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "210d44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(last_run_log_file_final_runs):\n",
    "    with open(last_run_log_file_final_runs, 'w+') as fw:\n",
    "        fw.write(\"Run log.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41ced5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    learning_rate=0.001\n",
    "    momentum=None\n",
    "    T_max=None\n",
    "    is_scheduler=False\n",
    "    optimizer_type=\"Adam\"\n",
    "    weight_decay=0.0001\n",
    "    smoothing=0.0371\n",
    "    scheduler=None\n",
    "    epochs=5000\n",
    "    criterion=nn.CrossEntropyLoss(weight=class_weights, label_smoothing=smoothing)\n",
    "    valid_criterion=nn.CrossEntropyLoss(weight=valid_class_weights)\n",
    "    lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "    decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "    mom_text=\"\"\n",
    "    scheduler_text=\"\"\n",
    "    for i in range(1000):\n",
    "        net = base_fc().to(device)\n",
    "        if optimizer_type==\"Adam\":\n",
    "            optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, \n",
    "                                        momentum=momentum, weight_decay=weight_decay)\n",
    "            mom_text=\"_mom\"+str(round(momentum,2)).replace(\".\",\"d\")\n",
    "        else:\n",
    "            raise RuntimeError(\"WRONG OPTIMIZER.\")\n",
    "        if is_scheduler:\n",
    "            scheduler=lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max*epochs)\n",
    "            scheduler_text=\"-cos\"+str(T_max).replace(\".\",\"d\")\n",
    "        run = wandb.init(project='mlp_basemodel_final')\n",
    "        run_name=\"run_{}\".format(i)\n",
    "        wandb.run.name = run_name\n",
    "        config = wandb.config\n",
    "        config.batch_size=train_batch_size\n",
    "        config.optimizer_type=optimizer_type\n",
    "        config.learning_rate = learning_rate\n",
    "        config.weight_decay=weight_decay\n",
    "        config.is_scheduler=is_scheduler\n",
    "        if is_scheduler:\n",
    "            config.T_max=T_max\n",
    "            config.scheduler=\"Cosine\"\n",
    "        if optimizer_type==\"SGD\":\n",
    "            config.momentum = momentum\n",
    "\n",
    "        greatest_acc = train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                          is_scheduler,scheduler, greatest_acc_overall, model_path, wandb, early_stop=None)\n",
    "\n",
    "        with open(last_run_log_file_final_runs, 'a+') as fw:\n",
    "                fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "                fw.write(\"Accuracy: {}\\n\".format(greatest_acc))\n",
    "                fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\nScheduler: {}\\n\".format(optimizer_type, learning_rate, is_scheduler))\n",
    "                fw.write(\"Weight decay: {}\\nSmoothing: {}\\nBatch size: {}\\n\".format(weight_decay, smoothing, train_batch_size))\n",
    "                if optimizer_type==\"SGD\":\n",
    "                    fw.write(\"Momentum: {}\\n\".format(momentum))\n",
    "                    if is_scheduler:\n",
    "                        fw.write(\"T_max: {}\\n\".format(T_max))\n",
    "                fw.write('\\n\\n')\n",
    "\n",
    "\n",
    "        if greatest_acc>greatest_acc_overall:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay, momentum, is_scheduler, T_max,\n",
    "                             smoothing, train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              greatest_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string)\n",
    "            greatest_acc_overall=greatest_acc\n",
    "                #raise Exception(\"LOL\")\n",
    "                        #run.finish()\n",
    "    #learning_rate=0.001\n",
    "    #momentum=0.9\n",
    "    #gamma=2\n",
    "    #is_scheduler=True\n",
    "    #optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)#, weight_decay=0.1-0.0001\n",
    "    #scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    #scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40375978",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1adc2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_skeleton(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(base_fc, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.dropout=nn.Dropout(0.3) #changed from 0.2->0.3 1.6.2022\n",
    "        self.fc1=nn.Linear(84,120)#150) \n",
    "        self.bn1 = nn.BatchNorm1d(120)\n",
    "        self.fc2=nn.Linear(120,8)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 84): Input sequences.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 8): Outputs of the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        y=self.fc1(x)\n",
    "        y=self.dropout(y)\n",
    "        y=F.relu(y)\n",
    "        y=self.bn1(y)\n",
    "        \n",
    "        y=self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b67a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_fc(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=84, out_features=120, bias=True)\n",
       "  (bn1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=120, out_features=8, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#model = model_skeleton(many_classes)\n",
    "model=base_fc()\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(os.path.join(\"saved_models\", \"basemodel_fc\", \"base_fc_v3.pth\")))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6a937fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TEST\n",
      "TP: 22 . FN: 76 TP/(TP+FN): 0.22448979591836735 TN: 17 FP: 81 TN/(TN+FP): 0.17346938775510204 Wrong positive class predicted: 64 Wrong negative class predicted: 15\n",
      "Fake F1-score: 0.6880000000000001 . Fake F2-score: 0.7904411764705882\n",
      "Fake TP/(TP+FN): 0.5308641975308642 Fake TN/(TN+FP) 0.2831858407079646\n",
      "Fake precision: 0.5657894736842105 Fake recall: 0.8775510204081632\n",
      "F1-score: 0.21890547263681592\n",
      "F2-score: 0.22222222222222224\n",
      "Precision: 0.21359223300970873\n",
      "Recall: 0.22448979591836735\n",
      "Fake accuracy: 0.6020408163265306\n",
      "Accuracy for the test data: 0.22930155249234196\n"
     ]
    }
   ],
   "source": [
    "criterion=nn.CrossEntropyLoss(weight=class_weights)\n",
    "accuracy, loss = \\\n",
    "compute_accuracy(device, model, testloader, None, \"TEST\", verbose=True, cv=False)\n",
    "\n",
    "print(\"Accuracy for the test data:\",accuracy)\n",
    "#print(\"Loss for test data:\",loss)\n",
    "# print(\"F1-score:\",f1)\n",
    "# print(\"F2-score:\",f2)\n",
    "# print(\"Precision:\",precision)\n",
    "# print(\"Recall:\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b88b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countProbabilityDistributions(net, dataloader):\n",
    "    distributions=dict()\n",
    "    m = nn.Softmax(dim=1)\n",
    "    predictions=None\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = net(sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if predictions==None:\n",
    "                predictions=predicted\n",
    "                trueLabels=labels\n",
    "            else:\n",
    "                predictions=torch.cat((predictions, predicted), 0)\n",
    "                trueLabels=torch.cat((trueLabels, labels), 0)\n",
    "            for i in range(outputs.data.shape[1]):\n",
    "                if not i in distributions:\n",
    "                    distributions[i]=m(outputs).data[:,i]\n",
    "                else:\n",
    "                    distributions[i]=torch.cat((distributions[i], m(outputs).data[:,i]), 0)\n",
    "                    #print(m(outputs).data[0,:])\n",
    "                    #print(predicted)\n",
    "                    #raise RuntimeError(\"LOL\")\n",
    "    return distributions, predictions, trueLabels\n",
    "#distributions, predictions, trueLabels= countProbabilityDistributions(net, trainloader)\n",
    "distributions, predictions, trueLabels= countProbabilityDistributions(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d878e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)\n",
    "plt.hist(predictions.to(\"cpu\").numpy(),bins=[0,1,2,3,4,5,6,7,8,9]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09039f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c96841",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames=['CA', 'CC', 'CG', 'CT', 'TA', 'TC', 'TG', 'TT']\n",
    "for i in range(8):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    condition=(predictions==i) & (trueLabels==i)\n",
    "    plt.hist(distributions[i][condition].to(\"cpu\").numpy(),bins=100, color='r', alpha=0.5);#, density=True);\n",
    "    condition=(predictions==i) & (trueLabels!=i)\n",
    "    plt.hist(distributions[i][condition].to(\"cpu\").numpy(),bins=100, color='b', alpha=0.5);#, density=True);\n",
    "    plt.title(classNames[i]);\n",
    "    plt.xlim([0,1])\n",
    "    plt.savefig('probDistr_{}.pdf'.format(classNames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)\n",
    "plt.figure(figsize=(20,20))\n",
    "condition=(predictions==1)\n",
    "plt.hist(distributions[1][condition].to(\"cpu\").numpy(),bins=100, color='r', alpha=0.5, density=True);\n",
    "other=torch.cat((distributions[0][predictions==0],distributions[2][predictions==2],distributions[3][predictions==3]),dim=0)\n",
    "plt.hist(other.to(\"cpu\").numpy(),bins=100, color='b', alpha=0.5, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,constrained_layout = True) # Instantiate figure and axes object\n",
    "ax[0][0].hist(distributions[0][predictions==0].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][0].title.set_text('CA 475')\n",
    "ax[0][1].hist(distributions[1][predictions==1].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][1].title.set_text('CC 3048')\n",
    "ax[1][0].hist(distributions[2][predictions==2].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][0].title.set_text('CG 127')\n",
    "ax[1][1].hist(distributions[3][predictions==3].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][1].title.set_text('CT 639')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ac37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,constrained_layout = True) # Instantiate figure and axes object\n",
    "ax[0][0].hist(distributions[4][predictions==4].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][0].title.set_text('TA 181')\n",
    "ax[0][1].hist(distributions[5][predictions==5].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][1].title.set_text('TC 303')\n",
    "ax[1][0].hist(distributions[6][predictions==6].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][0].title.set_text('TG 122')\n",
    "ax[1][1].hist(distributions[7][predictions==7].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][1].title.set_text('TT 4285')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c838178",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,4) # Instantiate figure and axes object\n",
    "for index in range(8):\n",
    "    ax[int(index>3)][index-(index>3)*4].hist(distributions[index].to(\"cpu\").numpy(), density=True, bins=20)#,\n",
    "            #bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\")\n",
    "    ax[int(index>3)][index-(index>3)*4].title.set_text(str(index))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,1) # Instantiate figure and axes object\n",
    "axes[0].hist(distributions[0].to(\"cpu\").numpy(), label=str(0),\n",
    "        bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\");\n",
    "axes[1].hist(distributions[1].to(\"cpu\").numpy(), label=str(1),\n",
    "        bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\");\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distributions[0][0:11].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, f1, f2, precision, recall, f1_fake, f2_fake, fake_precision, fake_recall = \\\n",
    "compute_accuracy(net, testloader, criterion, \"TEST\", True)\n",
    "\n",
    "print(\"Accuracy for the test data:\",accuracy)\n",
    "print(\"Loss for test data:\",loss)\n",
    "print(\"F1-score:\",f1)\n",
    "print(\"F2-score:\",f2)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "config.test_acc=accuracy\n",
    "config.test_loss=loss\n",
    "config.test_f1=f1\n",
    "config.test_f2=f2\n",
    "config.test_precis=precision\n",
    "config.test_recall=recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b29002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in testloader:\n",
    "    sequences, labels = sequences.to(device), labels.to(device)\n",
    "    outputs = net(sequences)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for sequence, label, prediction, output in zip(sequences, labels, predicted, outputs):\n",
    "        if label in [0,5,10,15]:\n",
    "            if label==prediction:\n",
    "                continue\n",
    "            else:\n",
    "                if prediction in [0,5,10,15]:\n",
    "                    print(class_types[prediction], class_types[label],sequence, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c694598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in testloader:\n",
    "    sequences, labels = sequences.to(device), labels.to(device)\n",
    "    outputs = net(sequences)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for sequence, label, prediction, output in zip(sequences, labels, predicted, outputs):\n",
    "        if prediction==label:\n",
    "            print(sequence,'\\n\\n',output,'\\n\\n', label,'\\n\\n', prediction, class_types[label.item()], \n",
    "                  class_types[prediction.item()])\n",
    "            #print(\"TRUE:\",class_types[prediction.item()])\n",
    "            raise UserWarning('Exit Early')\n",
    "        else:\n",
    "            print(\"FALSE\",sequence,'\\n\\n',output,'\\n\\n', label,'\\n\\n', prediction, class_types[label.item()],\n",
    "                 class_types[prediction.item()])\n",
    "            raise UserWarning('Exit Early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in trainloader:\n",
    "    for label in labels:\n",
    "        if class_types[label.item()]==\"TT\":\n",
    "            print(\"TT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor=torch.FloatTensor(np.array([[0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                [0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0],\n",
    "                                [0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                                [1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]]))\n",
    "test_target=torch.FloatTensor(8) #G->A=8\n",
    "test_tensor=test_tensor.reshape(1,1,4,33)\n",
    "out_test=net(test_tensor)\n",
    "print(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.argmax(out_test, 1)\n",
    "total =1\n",
    "correct = (predicted == torch.argmax(test_target)).sum().item()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.zeros(6)\n",
    "for luku in test:\n",
    "    print(luku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9399685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.FloatTensor(np.array([[0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                [0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0],\n",
    "                                [0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                                [1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = iter(testloader).next()\n",
    "    tests.plot_images(images[:5], n_rows=1)\n",
    "    \n",
    "    # Compute predictions\n",
    "    images = images.to(device)\n",
    "    y = net(images)\n",
    "\n",
    "print('Ground truth labels: ', ' '.join('%10s' % classes[labels[j]] for j in range(5)))\n",
    "print('Predictions:         ', ' '.join('%10s' % classes[j] for j in y.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beea5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the network on the test images: %.3f' % accuracy)\n",
    "assert accuracy > 0.85, \"Poor accuracy {:.3f}\".format(accuracy)\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
