{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c1621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import collections\n",
    "from math import log10, floor\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from process_input.input_to_2Dmatrix_transformer import parse_matrices, FeatureDataset, create_class_weights\n",
    "from model_skeletons.transformer.transformer_v12 import Transformer, model_version, WeightedMSELoss\n",
    "from compute_accuracy_transformer import compute_accuracy\n",
    "\n",
    "norm=True\n",
    "norm_type=None # \"sum\", \"max\"\n",
    "target_mode=\"whole_matrix\" #[\"whole_matrix\", \"whole_DNA_seq\", \"only_target_base\", \"target_with_landscape\"]\n",
    "if target_mode in [\"whole_matrix\", \"whole_DNA_seq\"]:\n",
    "    correct_label_index=16\n",
    "else:\n",
    "    correct_label_index=0\n",
    "model_script=os.path.join(\"model_skeletons\", \"transformer\", \"{}.py\".format(model_version)) #Used to read the file for logs\n",
    "assert os.path.isfile(model_script), \"Could not find file '{}'\".format(model_script)\n",
    "new_files_creation=\"create_new_data_files.py\" #Used to read the file for logs\n",
    "input_to_matrix=os.path.join(\"process_input\",\"input_to_2Dmatrix_transformer.py\") #Used to read the file for logs\n",
    "assert os.path.isfile(input_to_matrix), \"Could not find file '{}'\".format(input_to_matrix)\n",
    "compute_accuracy_file = \"compute_accuracy_transformer.py\"\n",
    "assert os.path.isfile(compute_accuracy_file), \"Could not find file '{}'\".format(compute_accuracy_file)\n",
    "valid_batch_size=5\n",
    "log_file=os.path.join(\"logs\", \"transformer\", \"{}.log\".format(model_version))\n",
    "cv_log_file=os.path.join(\"logs\", \"transformer\", \"cv_{}.log\".format(model_version))\n",
    "last_run_log_file=os.path.join(\"logs\", \"transformer\", \"{}_cv_all_runs.log\".format(model_version))\n",
    "last_run_log_file_final_runs=os.path.join(\"logs\", \"transformer\", \"{}_all_runs.log\".format(model_version))\n",
    "model_path=os.path.join(\"saved_models\", \"transformer\", \"{}.pth\".format(model_version))\n",
    "cv=False\n",
    "train=False\n",
    "indices_of_interest=[80,81,82,83]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda')\n",
    "many_classes=True #ACGT order in onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d00dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log_file(learning_rate, optimizer_type, weight_decay, \n",
    "                      train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                      input_to_matrix, new_greatest_valid_acc, model_path, many_classes,\n",
    "                      compute_accuracy_file, model_script, run_name, dt_string, target_mode,\n",
    "                      nhead, num_encoder_layers, num_decoder_layers, norm, norm_type, acc_list=None):\n",
    "    \"\"\"Called, when new highest validation accuracy is found.\n",
    "    \n",
    "    Writes everything important information to a log file.\"\"\"\n",
    "    with open(log_file, 'w+') as fw:\n",
    "        fw.write(\"Created: {}\\nModel version: {}\\nPath: {}\\nRun name: {}\\nAccuracy: {}\\n\\n\".format(dt_string, model_version, \n",
    "                                                                                                   model_path, run_name, \n",
    "                                                                                                   new_greatest_valid_acc))\n",
    "        if acc_list:\n",
    "            for k, accuracy in enumerate(acc_list):\n",
    "                fw.write(\"Fold {}: {}.\\n\".format(k+1, accuracy))\n",
    "            fw.write('\\n')\n",
    "        fw.write(\"Hyperparameters:\\nOptimizer: {}\\n\".format(optimizer_type))\n",
    "        fw.write(\"Learning rate: {}\\nWeight decay: {}\\n\".format(learning_rate, weight_decay))\n",
    "        fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "        fw.write(\"Used MSELoss\\n\")\n",
    "        fw.write(\"Balanced classes\\n\")\n",
    "        fw.write(\"Data normalized: {}\\n\".format(norm))\n",
    "        fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "        fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "        fw.write(\"Train batch size: {}\\nValidation batch size: {}\\n\\n\".format(train_batch_size, valid_batch_size))\n",
    "        \n",
    "        fw.write(\"\\n--------------------------Script of the model can be seen below.---------------------------\\n\")\n",
    "        with open(model_script, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "    \n",
    "        fw.write(\"\\n------------------------------Created input matrices with script:------------------------------\\n\")\n",
    "        with open(input_to_matrix, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\")\n",
    "        fw.write(\"\\n\\n\\n\")\n",
    "        \n",
    "        fw.write(\"\\n------------------------------Computed accuracies with script:-----------------------------\\n\")\n",
    "        with open(compute_accuracy_file, 'r') as fr:\n",
    "            fw.write(fr.read())\n",
    "        fw.write(\"\\n-------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c28e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In input file, 33 base, 17th is altering, 1st A, 2nd C, 3rd G, 4th T\"\"\"\n",
    "class_types=['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG',\n",
    "            'GT', 'TA', 'TC', 'TG', 'TT']\n",
    "classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44bc2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_train.matrix\"\n",
    "valid_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_valid.matrix\"\n",
    "test_file=\"data/20220214/sompred_crc9_clu1_pyri_mut_combined_test.matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1153f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_input, trainset_target, class_weights_train_whole = parse_matrices(train_file, norm, target_mode, norm_type)\n",
    "valid_input, valid_target, class_weights_valid_whole = parse_matrices(valid_file, norm, target_mode, norm_type)\n",
    "test_input, test_target, class_weights_test = parse_matrices(test_file, norm, target_mode, norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c303f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.zeros((1, 1, trainset_input.shape[2]))\n",
    "eos_token = torch.ones((1, 1, trainset_input.shape[2]))\n",
    "\n",
    "trainset_input_with_tokens = torch.cat([trainset_input, eos_token.expand(trainset_input.size(0), 1, -1)], dim=1)\n",
    "trainset_target_with_tokens = torch.cat([sos_token.expand(trainset_input.size(0), 1, -1), trainset_target, eos_token.expand(trainset_input.size(0), 1, -1)], dim=1)\n",
    "\n",
    "valid_input_with_tokens = torch.cat([valid_input, eos_token.expand(valid_input.size(0), 1, -1)], dim=1)\n",
    "valid_target_with_tokens = torch.cat([sos_token.expand(valid_input.size(0), 1, -1), valid_target, eos_token.expand(valid_input.size(0), 1, -1)], dim=1)\n",
    "\n",
    "test_input_with_tokens = torch.cat([test_input, eos_token.expand(test_input.size(0), 1, -1)], dim=1)\n",
    "test_target_with_tokens = torch.cat([sos_token.expand(test_input.size(0), 1, -1), test_target, eos_token.expand(test_input.size(0), 1, -1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2feed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=FeatureDataset(data=trainset_input_with_tokens, labels=trainset_target_with_tokens)\n",
    "valid_dataset=FeatureDataset(data=valid_input_with_tokens, labels=valid_target_with_tokens)\n",
    "test_dataset=FeatureDataset(data=test_input_with_tokens, labels=test_target_with_tokens)\n",
    "combined_valid_train = ConcatDataset([train_dataset, valid_dataset]) #Combines validation and training datasets for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f99f4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  correct_label_index, wandb, early_stop=100):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    min_tot_loss=float('inf')\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            labels_input = labels[:,:-1]\n",
    "            labels_expected = labels[:,1:]\n",
    "            \n",
    "            sequence_length = labels_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(sequence_length, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = net(sequences, labels_input, tgt_mask =tgt_mask)\n",
    "            #out = out.squeeze()\n",
    "            loss=criterion(sequences, labels_expected, out)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \n",
    "                                                \"VALID\", verbose = False, cv=True,\n",
    "                                                   correct_label_index=correct_label_index) #17th/33+Start token\n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "        if round(accuracy,3)<=round(greatest_acc,3):\n",
    "            pass\n",
    "        else:\n",
    "            greatest_acc=accuracy\n",
    "        if round(tot_valid_loss,3)>=round(min_tot_loss,3):\n",
    "            j+=1\n",
    "            if j>=early_stop and i>100:\n",
    "                break\n",
    "        else:\n",
    "            j=0\n",
    "            min_tot_loss=tot_valid_loss\n",
    "        \n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dff9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                  correct_label_index, greatest_acc_overall, model_path, wandb, early_stop=50):\n",
    "    j=0\n",
    "    greatest_acc=0\n",
    "    tot_loss=0\n",
    "    tot_items=0\n",
    "    for i in range(epochs):\n",
    "        net.train()\n",
    "        for sequences, labels in trainloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            labels_input = labels[:,:-1]\n",
    "            labels_expected = labels[:,1:]\n",
    "            \n",
    "            sequence_length = labels_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(sequence_length, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = net(sequences, labels_input, tgt_mask =tgt_mask)\n",
    "            #out=out.squeeze()\n",
    "            loss=criterion(sequences, labels_expected, out)\n",
    "            tot_loss+=loss.item()\n",
    "            tot_items+=len(labels)\n",
    "            loss.backward()\n",
    "            if torch.isnan(loss):\n",
    "                raise RuntimeError(\"NAN!\")\n",
    "            optimizer.step()\n",
    "        tot_loss/=tot_items\n",
    "        accuracy, tot_valid_loss = compute_accuracy(device, net, validloader, valid_criterion, \"VALID\", \n",
    "                                                    verbose = False, cv = True, correct_label_index=correct_label_index) #17th/33+1\n",
    "        \n",
    "        if wandb!=None:\n",
    "            wandb.log({\"Training loss\": tot_loss,\n",
    "                       \"Validation loss\": tot_valid_loss,\n",
    "                       \"Valid Accuracy\": accuracy,\n",
    "            #           \"Test loss\": test_loss,\n",
    "            #           \"Test Accuracy\": test_accuracy,\n",
    "            #           \"Pooled test recall\": fake_recall_test,\n",
    "            #           \"Pooled test precision\": fake_precision_test,\n",
    "            #           \"Learning rate\": optimizer.param_groups[0]['lr'],\n",
    "            #           \"Scheduler\": is_scheduler,\n",
    "                       \"Epoch\": i})\n",
    "            \n",
    "        if round(accuracy,4)<=round(greatest_acc,4):\n",
    "            if early_stop:\n",
    "                j+=1\n",
    "                if j>=early_stop and i>150:\n",
    "                    break\n",
    "        else:\n",
    "            if accuracy>greatest_acc_overall:\n",
    "                torch.save(net.state_dict(), model_path)\n",
    "                greatest_acc_overall=accuracy\n",
    "            j=0\n",
    "            greatest_acc=accuracy\n",
    "    print(\"Greatest accuracy on the run: {}\".format(greatest_acc))\n",
    "    return greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c2d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earlier_accuracy(log_file):\n",
    "    with open(log_file, 'r') as fr:\n",
    "        for line in fr:\n",
    "            if \"Accuracy:\" in line:\n",
    "                return float(line.strip().split(' ')[1]) #Accuracy is written as Accuracy: <acc>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20311664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3483568324723381\n"
     ]
    }
   ],
   "source": [
    "greatest_avg_valid_acc = 0\n",
    "if os.path.isfile(cv_log_file):\n",
    "    greatest_avg_valid_acc = get_earlier_accuracy(cv_log_file)\n",
    "print(greatest_avg_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb262fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(last_run_log_file):\n",
    "    with open(last_run_log_file, 'w+') as fw:\n",
    "        fw.write(\"Run log.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8257a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(device, dataloader, indices_of_interest, norm_type):\n",
    "    total_amount=0\n",
    "    class_amounts=collections.Counter()\n",
    "    bases = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    for sequences, labels in dataloader:\n",
    "        values_of_interest = sequences[:, 16, indices_of_interest]\n",
    "        _, original_bases = torch.max(values_of_interest, dim=1)\n",
    "\n",
    "        values_of_interest = labels[:, correct_label_index, indices_of_interest] #\n",
    "        _, new_bases = torch.max(values_of_interest, dim=1)\n",
    "        for new_base, original_base in zip(new_bases, original_bases):\n",
    "            correct_class = classes[bases[original_base.item()]+bases[new_base.item()]]\n",
    "            class_amounts[correct_class]+=1\n",
    "            total_amount+=1\n",
    "            \n",
    "    return create_class_weights(class_amounts, total_amount, norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22ddd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cv:\n",
    "    k_folds=5\n",
    "    epochs=5000\n",
    "    seq_len=33\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True) #batch size affects the size of datasets\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    for i in range(150): #Test with 150 different hyperparameter combinations\n",
    "        valid_accuracies = list()\n",
    "        learning_rate=random.sample([0.0001, 0.00001, 0.000001], 1)[0]\n",
    "        lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "        train_batch_size=random.sample([32, 64, 128, 256], 1)[0]\n",
    "        norm_type = random.sample([\"sum\", \"max\", \"None\"], 1)[0]\n",
    "        \n",
    "        nhead=random.sample([2, 3, 4, 6, 7, 12, 21, 42], 1)[0]\n",
    "        num_encoder_layers=random.sample([2, 3, 4, 6, 8], 1)[0]\n",
    "        num_decoder_layers=num_encoder_layers\n",
    "        \n",
    "        optimizer_type=random.sample([\"Adam\",\"AdamW\"], 1)[0] #random.sample([\"Adam\",\"SGD\"], 1)[0]\n",
    "        weight_decay=random.sample([0, 0.000001, 0.00000001], 1)[0]\n",
    "        if weight_decay!=0: weight_decay = round(weight_decay, -int(floor(log10(weight_decay))) + 2)\n",
    "        decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "        for (train_ids, test_ids) in kfold.split(combined_valid_train):\n",
    "            \n",
    "            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "            test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                          combined_valid_train, \n",
    "                          batch_size=train_batch_size, sampler=train_subsampler)\n",
    "            validloader = torch.utils.data.DataLoader(\n",
    "                              combined_valid_train,\n",
    "                              batch_size=valid_batch_size, sampler=test_subsampler)\n",
    "            train_class_weights = get_class_weights(device, trainloader, indices_of_interest, norm_type)\n",
    "            valid_class_weights = get_class_weights(device, validloader, indices_of_interest, norm_type)\n",
    "            \n",
    "            net = Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers).to(device)\n",
    "            if optimizer_type==\"Adam\":\n",
    "                optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            elif optimizer_type==\"AdamW\":\n",
    "                optimizer=torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            else:\n",
    "                raise RuntimeError(\"WRONG OPTIMIZER: {}\".format(optimizer_type))\n",
    "            criterion = WeightedMSELoss(device, train_class_weights, classes, correct_label_index, indices_of_interest)\n",
    "            valid_criterion=WeightedMSELoss(device, valid_class_weights, classes, correct_label_index, indices_of_interest)\n",
    "            run_name = None\n",
    "            valid_acc = cv_train_network(net,criterion,valid_criterion,epochs,optimizer,trainloader, validloader, \n",
    "                                      correct_label_index, wandb=None)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "        avg_valid_acc = sum(valid_accuracies) / len(valid_accuracies)\n",
    "\n",
    "        print(\"Optimizer: {}\\nLearning Rate: {}\".format(optimizer_type, learning_rate))\n",
    "        print(\"Weight decay: {}\".format(weight_decay))\n",
    "        print(\"Average validation accuracy: {}\".format(avg_valid_acc))\n",
    "        for k, accuracy in enumerate(valid_accuracies):\n",
    "            print(\"Fold {}: {}.\".format(k+1, accuracy))\n",
    "\n",
    "        with open(last_run_log_file, 'a+') as fw:\n",
    "            fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "            fw.write(\"Average validation accuracy: {}\\n\".format(avg_valid_acc))\n",
    "            fw.write(\"\\n\".join([\"Fold {}: {}.\".format(k+1, accuracy) for k, accuracy in enumerate(valid_accuracies)]))\n",
    "            fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\n\".format(optimizer_type, learning_rate))\n",
    "            fw.write(\"Weight decay: {}\\nBatch size: {}\\n\".format(weight_decay, train_batch_size))\n",
    "            fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "            fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "            fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "            fw.write('\\n\\n')\n",
    "\n",
    "        if avg_valid_acc>greatest_avg_valid_acc:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay,\n",
    "                             train_batch_size, valid_batch_size, cv_log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              avg_valid_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string, target_mode, \n",
    "                              nhead, num_encoder_layers, num_decoder_layers, acc_list=valid_accuracies, norm=norm,\n",
    "                                 norm_type=norm_type)\n",
    "            greatest_avg_valid_acc=avg_valid_acc\n",
    "            os.path.join(\"saved_models\", \"transformer\", \"test_notebook.pth\")\n",
    "            torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b04a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49964248632637526\n"
     ]
    }
   ],
   "source": [
    "greatest_acc_overall=0\n",
    "if os.path.isfile(log_file):\n",
    "    greatest_acc_overall=get_earlier_accuracy(log_file)\n",
    "print(greatest_acc_overall)\n",
    "train_batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a81bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset\n",
    "    ,batch_size=train_batch_size\n",
    "    ,shuffle=True\n",
    "    ,drop_last=True\n",
    ")\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37f01f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    learning_rate=0.0001\n",
    "    optimizer_type=\"AdamW\"\n",
    "    weight_decay=0.000001\n",
    "    nhead=42\n",
    "    num_encoder_layers=6\n",
    "    num_decoder_layers=6\n",
    "    epochs=5000\n",
    "    norm_type=\"None\"\n",
    "    criterion = WeightedMSELoss(device, class_weights_train_whole, classes, correct_label_index, indices_of_interest)\n",
    "    valid_criterion=WeightedMSELoss(device, class_weights_valid_whole, classes, correct_label_index, indices_of_interest)\n",
    "    lr_text=str(learning_rate).replace(\".\",\"d\")\n",
    "    decay_text=\"_wdecay\"+str(weight_decay).replace(\".\",\"d\")\n",
    "    for i in range(1000):\n",
    "        net = Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers).to(device)\n",
    "        if optimizer_type==\"Adam\":\n",
    "            optimizer=torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type==\"AdamW\":\n",
    "            optimizer=torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise RuntimeError(\"WRONG OPTIMIZER.\")\n",
    "        run = wandb.init(project='transformer_final')\n",
    "        run_name=\"run_{}\".format(i)\n",
    "        wandb.run.name = run_name\n",
    "        config = wandb.config\n",
    "        config.batch_size=train_batch_size\n",
    "        config.optimizer_type=optimizer_type\n",
    "        config.learning_rate = learning_rate\n",
    "        config.weight_decay=weight_decay\n",
    "        config.is_scheduler=is_scheduler\n",
    "        if is_scheduler:\n",
    "            config.T_max=T_max\n",
    "            config.scheduler=\"Cosine\"\n",
    "        if optimizer_type==\"SGD\":\n",
    "            config.momentum = momentum\n",
    "\n",
    "        greatest_acc = train_network(net,criterion, valid_criterion,epochs,optimizer,trainloader, validloader,\n",
    "                          correct_label_index, greatest_acc_overall, model_path, wandb=None, early_stop=100)\n",
    "                \n",
    "        with open(last_run_log_file_final_runs, 'a+') as fw:\n",
    "            fw.write(\"Time: {}\\n\".format(datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")))\n",
    "            fw.write(\"Validation accuracy: {}\\n\".format(greatest_acc))\n",
    "            fw.write(\"\\nOptimizer: {}\\nLearning Rate: {}\\n\".format(optimizer_type, learning_rate))\n",
    "            fw.write(\"Weight decay: {}\\nBatch size: {}\\n\".format(weight_decay, train_batch_size))\n",
    "            fw.write(\"head: {}\\nnum_encoder_layers: {}\\nnum_decoder_layers: {}\\n\".format(nhead, num_encoder_layers, num_decoder_layers))\n",
    "            fw.write(\"Norm type: {}\\n\".format(norm_type))\n",
    "            fw.write(\"Target mode: {}\\n\".format(target_mode))\n",
    "            fw.write('\\n\\n')\n",
    "       \n",
    "        \n",
    "        if greatest_acc>greatest_acc_overall:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            write_to_log_file(learning_rate, optimizer_type, weight_decay,\n",
    "                             train_batch_size, valid_batch_size, log_file, model_version, \n",
    "                              input_to_matrix, \n",
    "                              greatest_acc, model_path, many_classes, compute_accuracy_file,\n",
    "                              model_script, run_name, dt_string, target_mode, \n",
    "                              nhead, num_encoder_layers, num_decoder_layers, norm=norm,\n",
    "                                 norm_type=norm_type)\n",
    "            greatest_acc_overall=greatest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd0193a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(test_dataset\n",
    "    ,batch_size=5\n",
    "    ,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09059ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (positional_encoder): PositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=84, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=84, bias=True)\n",
       "          (norm1): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=84, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=84, bias=True)\n",
       "          (norm1): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=84, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhead=42\n",
    "num_encoder_layers=6\n",
    "num_decoder_layers=6\n",
    "\n",
    "model=Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model_path = os.path.join(\"saved_models\", \"transformer\", \"transformer_v10_masters_thesis.pth\".format(model_version))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8f25d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/77/jarvint12/unix/anaconda3/envs/jupyter2/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CA: Correct 10, total 16, acc 0.625\n",
      "Class CC: Correct 29, total 42, acc 0.6904761904761905\n",
      "Class CG: Correct 3, total 9, acc 0.3333333333333333\n",
      "Class CT: Correct 10, total 19, acc 0.5263157894736842\n",
      "Class TA: Correct 5, total 14, acc 0.35714285714285715\n",
      "Class TC: Correct 1, total 13, acc 0.07692307692307693\n",
      "Class TG: Correct 15, total 27, acc 0.5555555555555556\n",
      "Class TT: Correct 30, total 56, acc 0.5357142857142857\n",
      "\n",
      " TEST\n",
      "TP: 44 . FN: 54 TP/(TP+FN): 0.4489795918367347 TN: 59 FP: 39 TN/(TN+FP): 0.6020408163265306 Wrong positive class predicted: 42 Wrong negative class predicted: 4\n",
      "Fake F1-score: 0.7853881278538812 . Fake F2-score: 0.8382066276803118\n",
      "Fake TP/(TP+FN): 0.6142857142857143 Fake TN/(TN+FP) 0.6176470588235294\n",
      "Fake precision: 0.7107438016528925 Fake recall: 0.8775510204081632\n",
      "F1-score: 0.48618784530386744\n",
      "F2-score: 0.4631578947368421\n",
      "Precision: 0.5301204819277109\n",
      "Recall: 0.4489795918367347\n",
      "Fake accuracy: 0.7602040816326531\n",
      "Test accuracy: 0.4625576360773729\n"
     ]
    }
   ],
   "source": [
    "criterion=WeightedMSELoss(device, class_weights_test, classes, correct_label_index, indices_of_interest)\n",
    "accuracy, tot_valid_loss = compute_accuracy(device, model, testloader, None, \"TEST\", \n",
    "                                                    verbose = True, cv = False, correct_label_index=correct_label_index)\n",
    "print(\"Test accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d185d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CA: Correct 35, total 84, acc 0.4166666666666667\n",
      "Class CC: Correct 325, total 545, acc 0.5963302752293578\n",
      "Class CG: Correct 10, total 22, acc 0.45454545454545453\n",
      "Class CT: Correct 63, total 112, acc 0.5625\n",
      "Class TA: Correct 18, total 31, acc 0.5806451612903226\n",
      "Class TC: Correct 17, total 53, acc 0.32075471698113206\n",
      "Class TG: Correct 12, total 21, acc 0.5714285714285714\n",
      "Class TT: Correct 369, total 758, acc 0.4868073878627968\n",
      "\n",
      " VALID\n",
      "TP: 155 . FN: 168 TP/(TP+FN): 0.47987616099071206 TN: 694 FP: 609 TN/(TN+FP): 0.5326170376055257 Wrong positive class predicted: 115 Wrong negative class predicted: 104\n",
      "Fake F1-score: 0.4918032786885246 . Fake F2-score: 0.6531204644412192\n",
      "Fake TP/(TP+FN): 0.6164383561643836 Fake TN/(TN+FP) 0.5671641791044776\n",
      "Fake precision: 0.34838709677419355 Fake recall: 0.8359133126934984\n",
      "F1-score: 0.28518859245630174\n",
      "F2-score: 0.3769455252918288\n",
      "Precision: 0.20287958115183247\n",
      "Recall: 0.47987616099071206\n",
      "Fake accuracy: 0.6568265682656826\n",
      "Valid accuracy: 0.4987097792505377\n"
     ]
    }
   ],
   "source": [
    "accuracy, tot_valid_loss = compute_accuracy(device, model, validloader, None, \"VALID\", \n",
    "                                                    verbose = True, cv = False, correct_label_index=correct_label_index)\n",
    "print(\"Valid accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99fbc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_target, class_weights_test = parse_matrices(test_file, norm, target_mode, norm_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fc7dd18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset\u001b[38;5;241m=\u001b[39mFeatureDataset(data\u001b[38;5;241m=\u001b[39mtest_input, labels\u001b[38;5;241m=\u001b[39mtest_target)\n\u001b[0;32m----> 2\u001b[0m testloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(test_dataset\n\u001b[1;32m      3\u001b[0m                            ,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      4\u001b[0m                             ,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset=FeatureDataset(data=test_input, labels=test_target)\n",
    "testloader = DataLoader(test_dataset\n",
    "                           ,batch_size=5\n",
    "                            ,shuffle=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb70066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/u/77/jarvint12/unix/huslab_timo_dev/masters_thesis/saved_models/transformer/{}.pth\".format(model_version)\n",
    "nhead=42\n",
    "num_encoder_layers=6\n",
    "num_decoder_layers=6\n",
    "model=Transformer(nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(model_path)\n",
    "accuracy, tot_valid_loss = compute_accuracy(device, model, testloader, None, \"TEST\", \n",
    "                                                    verbose = True, cv = False, correct_label_index=correct_label_index)\n",
    "print(\"Test accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b88b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countProbabilityDistributions(net, dataloader):\n",
    "    distributions=dict()\n",
    "    x=None\n",
    "    m = nn.Softmax(dim=1)\n",
    "    predictions=None\n",
    "    with torch.no_grad():\n",
    "        tot_loss=0\n",
    "        for sequences, labels in dataloader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = net(sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if predictions==None:\n",
    "                predictions=predicted\n",
    "                trueLabels=labels\n",
    "            else:\n",
    "                predictions=torch.cat((predictions, predicted), 0)\n",
    "                trueLabels=torch.cat((trueLabels, labels), 0)\n",
    "            for i in range(outputs.data.shape[1]):\n",
    "                if not i in distributions:\n",
    "                    distributions[i]=m(outputs).data[:,i]\n",
    "                else:\n",
    "                    distributions[i]=torch.cat((distributions[i], m(outputs).data[:,i]), 0)\n",
    "                    #print(m(outputs).data[0,:])\n",
    "                    #print(predicted)\n",
    "                    #raise RuntimeError(\"LOL\")\n",
    "    return distributions, predictions, trueLabels\n",
    "#distributions, predictions, trueLabels= countProbabilityDistributions(net, trainloader)\n",
    "distributions, predictions, trueLabels= countProbabilityDistributions(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d878e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)\n",
    "plt.hist(predictions.to(\"cpu\").numpy(),bins=[0,1,2,3,4,5,6,7,8,9]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09039f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c96841",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames=['CA', 'CC', 'CG', 'CT', 'TA', 'TC', 'TG', 'TT']\n",
    "for i in range(8):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    condition=(predictions==i) & (trueLabels==i)\n",
    "    plt.hist(distributions[i][condition].to(\"cpu\").numpy(),bins=100, color='r', alpha=0.5);#, density=True);\n",
    "    condition=(predictions==i) & (trueLabels!=i)\n",
    "    plt.hist(distributions[i][condition].to(\"cpu\").numpy(),bins=100, color='b', alpha=0.5);#, density=True);\n",
    "    plt.title(classNames[i]);\n",
    "    plt.xlim([0,1])\n",
    "    plt.savefig('probDistr_{}.pdf'.format(classNames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)\n",
    "plt.figure(figsize=(20,20))\n",
    "condition=(predictions==1)\n",
    "plt.hist(distributions[1][condition].to(\"cpu\").numpy(),bins=100, color='r', alpha=0.5, density=True);\n",
    "other=torch.cat((distributions[0][predictions==0],distributions[2][predictions==2],distributions[3][predictions==3]),dim=0)\n",
    "plt.hist(other.to(\"cpu\").numpy(),bins=100, color='b', alpha=0.5, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,constrained_layout = True) # Instantiate figure and axes object\n",
    "ax[0][0].hist(distributions[0][predictions==0].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][0].title.set_text('CA 475')\n",
    "ax[0][1].hist(distributions[1][predictions==1].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][1].title.set_text('CC 3048')\n",
    "ax[1][0].hist(distributions[2][predictions==2].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][0].title.set_text('CG 127')\n",
    "ax[1][1].hist(distributions[3][predictions==3].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][1].title.set_text('CT 639')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ac37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,constrained_layout = True) # Instantiate figure and axes object\n",
    "ax[0][0].hist(distributions[4][predictions==4].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][0].title.set_text('TA 181')\n",
    "ax[0][1].hist(distributions[5][predictions==5].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[0][1].title.set_text('TC 303')\n",
    "ax[1][0].hist(distributions[6][predictions==6].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][0].title.set_text('TG 122')\n",
    "ax[1][1].hist(distributions[7][predictions==7].to(\"cpu\").numpy(),bins=100, color='r', density=True);\n",
    "ax[1][1].title.set_text('TT 4285')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c838178",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,4) # Instantiate figure and axes object\n",
    "for index in range(8):\n",
    "    ax[int(index>3)][index-(index>3)*4].hist(distributions[index].to(\"cpu\").numpy(), density=True, bins=20)#,\n",
    "            #bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\")\n",
    "    ax[int(index>3)][index-(index>3)*4].title.set_text(str(index))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,1) # Instantiate figure and axes object\n",
    "axes[0].hist(distributions[0].to(\"cpu\").numpy(), label=str(0),\n",
    "        bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\");\n",
    "axes[1].hist(distributions[1].to(\"cpu\").numpy(), label=str(1),\n",
    "        bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], density=True, histtype=\"step\");\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distributions[0][0:11].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, f1, f2, precision, recall, f1_fake, f2_fake, fake_precision, fake_recall = \\\n",
    "compute_accuracy(device, net, testloader, criterion, many_classes, \"TEST\", True, correct_label_index=1)\n",
    "\n",
    "print(\"Accuracy for the test data:\",accuracy)\n",
    "print(\"Loss for test data:\",loss)\n",
    "print(\"F1-score:\",f1)\n",
    "print(\"F2-score:\",f2)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "config.test_acc=accuracy\n",
    "config.test_loss=loss\n",
    "config.test_f1=f1\n",
    "config.test_f2=f2\n",
    "config.test_precis=precision\n",
    "config.test_recall=recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b29002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in testloader:\n",
    "    sequences, labels = sequences.to(device), labels.to(device)\n",
    "    outputs = net(sequences)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for sequence, label, prediction, output in zip(sequences, labels, predicted, outputs):\n",
    "        if label in [0,5,10,15]:\n",
    "            if label==prediction:\n",
    "                continue\n",
    "            else:\n",
    "                if prediction in [0,5,10,15]:\n",
    "                    print(class_types[prediction], class_types[label],sequence, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c694598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in testloader:\n",
    "    sequences, labels = sequences.to(device), labels.to(device)\n",
    "    outputs = net(sequences)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for sequence, label, prediction, output in zip(sequences, labels, predicted, outputs):\n",
    "        if prediction==label:\n",
    "            print(sequence,'\\n\\n',output,'\\n\\n', label,'\\n\\n', prediction, class_types[label.item()], \n",
    "                  class_types[prediction.item()])\n",
    "            #print(\"TRUE:\",class_types[prediction.item()])\n",
    "            raise UserWarning('Exit Early')\n",
    "        else:\n",
    "            print(\"FALSE\",sequence,'\\n\\n',output,'\\n\\n', label,'\\n\\n', prediction, class_types[label.item()],\n",
    "                 class_types[prediction.item()])\n",
    "            raise UserWarning('Exit Early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequences, labels in trainloader:\n",
    "    for label in labels:\n",
    "        if class_types[label.item()]==\"TT\":\n",
    "            print(\"TT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor=torch.FloatTensor(np.array([[0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                [0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0],\n",
    "                                [0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                                [1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]]))\n",
    "test_target=torch.FloatTensor(8) #G->A=8\n",
    "test_tensor=test_tensor.reshape(1,1,4,33)\n",
    "out_test=net(test_tensor)\n",
    "print(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.argmax(out_test, 1)\n",
    "total =1\n",
    "correct = (predicted == torch.argmax(test_target)).sum().item()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.zeros(6)\n",
    "for luku in test:\n",
    "    print(luku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9399685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.FloatTensor(np.array([[0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                [0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0],\n",
    "                                [0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "                                [1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = iter(testloader).next()\n",
    "    tests.plot_images(images[:5], n_rows=1)\n",
    "    \n",
    "    # Compute predictions\n",
    "    images = images.to(device)\n",
    "    y = net(images)\n",
    "\n",
    "print('Ground truth labels: ', ' '.join('%10s' % classes[labels[j]] for j in range(5)))\n",
    "print('Predictions:         ', ' '.join('%10s' % classes[j] for j in y.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beea5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the network on the test images: %.3f' % accuracy)\n",
    "assert accuracy > 0.85, \"Poor accuracy {:.3f}\".format(accuracy)\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
