Model version: cnn_v1_1
Path: models/cnn_v1_1.pth
Run name: batch128_lr-0d01smooth0d0optim-SGD_wdecay1e-05_sch-False_mom0d86
Accuracy: 0.44663099759848934

Hyperparameters:
Optimizer: SGD
Momentum: 0.8611482317699584
Learning rate: 0.01
Weight decay: 1e-05
Used CrossEntropyLoss with label smoothing 0.0009794280506340292
Balanced classes
Train batch size: 128
Validation batch size: 5


--------------------------Script of the model can be seen below.---------------------------
import torch.nn as nn
import torch.nn.functional as F

class cnn(nn.Module):
    def __init__(self, many_classes):
        super(cnn, self).__init__()
        # YOUR CODE HERE
        self.many_classes=many_classes
        self.dropout=nn.Dropout(0.3)
        self.m = nn.MaxPool1d(2, stride=2)
       # self.norm0=nn.BatchNorm2d(21)
        #self.conv1 = nn.Conv1d(1, 6, (4,1))
        self.conv1 = nn.Conv1d(21, 6, (4,1))
        #self.norm1=nn.BatchNorm2d(6)
        self.conv2 = nn.Conv1d(6, 12, 3)
        #self.norm2=nn.BatchNorm2d(12)
        self.conv3 = nn.Conv1d(12, 15, 3)
        #self.norm3=nn.BatchNorm2d(15)
        self.conv4 = nn.Conv1d(10, 10, 5)
        #self.norm4=nn.BatchNorm2d(10)
        self.conv1_2 = nn.Conv1d(21, 15, (4,33))
        #self.norm1_2=nn.BatchNorm2d(15)
        self.fc1=nn.Linear(435,200)#150) 
        #self.bn1 = nn.BatchNorm1d(200)
        self.fc2=nn.Linear(200,120)
        #self.bn2 = nn.BatchNorm1d(120)
        self.fc3=nn.Linear(120,8)
        self.fc4=nn.Linear(120,1)
        self.sigmoid=nn.Sigmoid()
        

    def forward(self, x):
        """
        Args:
          x of shape (batch_size, 1, 28, 28): Input images.
        
        Returns:
          y of shape (batch_size, 10): Outputs of the network.
        """
        y=F.relu(self.conv1(x))
        y=y.squeeze(dim=2)
        #y=self.m(y)
       # y=y.unsqueeze(dim=2)
        y=F.relu(self.conv2(y))
        #y=y.squeeze(dim=2)
        #y=self.m(y)
        #y=y.unsqueeze(dim=2)
        y=self.dropout(y)
        y=F.relu(self.conv3(y))
        #y=y.squeeze(dim=2)
       # y=self.m(y)
        y=self.dropout(y)
        #y=F.relu(self.conv4(y))
        #self.dropout(y)
        #y2=F.relu(self.conv1_2(x)).squeeze(dim=2)
        #y=torch.cat((y,y2), dim=2)
        #y=self.dropout(y)
        y = y.view(-1, self.num_flat_features(y))
        y=F.relu(self.fc1(y))
        y=self.dropout(y)
        y=F.relu(self.fc2(y))
        y=self.dropout(y)
        if self.many_classes:
            y=self.fc3(y)
        #else:
        #    y=self.fc4(y)
        #    y=self.sigmoid(y)
        return y
    
    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
-------------------------------------------------------------------------------------------



---------------------------Normalized input data. Script below.----------------------------
import torch

def normalize(trainset_input, valid_input, test_input):
    """Normalizes by each channel's (4x33) maximum value"""
    for i in range(trainset_input.shape[1]):
        if i==0:
            trainset_input2=(trainset_input[:,i,:,:]/trainset_input[:,i,:,:].max()).reshape(trainset_input.shape[0],
                                                                                     1, trainset_input.shape[2],
                                                                                         trainset_input.shape[3])
            print("Training set original shape: {}.\nTraining set normalized shape for one channel: {}.".format(trainset_input.shape,trainset_input2.shape))
        else:
            temp=(trainset_input[:,i,:,:]/trainset_input[:,i,:,:].max()).reshape(trainset_input.shape[0],
                                                                                     1, trainset_input.shape[2],
                                                                                         trainset_input.shape[3])
            trainset_input2=torch.cat((trainset_input2,temp), dim=1)

    for i in range(valid_input.shape[1]):
        if i==0:
            valid_input2=(valid_input[:,i,:,:]/valid_input[:,i,:,:].max()).reshape(valid_input.shape[0],
                                                                                     1, valid_input.shape[2],
                                                                                         valid_input.shape[3])
            print("Training set original shape: {}.\nTraining set normalized shape for one channel: {}.".format(valid_input.shape,valid_input2.shape))
        else:
            temp=(valid_input[:,i,:,:]/valid_input[:,i,:,:].max()).reshape(valid_input.shape[0],
                                                                                     1, valid_input.shape[2],
                                                                                         valid_input.shape[3])
            valid_input2=torch.cat((valid_input2,temp), dim=1)


    for i in range(test_input.shape[1]):
        if i==0:
            test_input2=(test_input[:,i,:,:]/test_input[:,i,:,:].max()).reshape(test_input.shape[0],
                                                                                     1, test_input.shape[2],
                                                                                         test_input.shape[3])
            print("Training set original shape: {}.\nTraining set normalized shape for one channel: {}.".format(test_input.shape,test_input2.shape))
        else:
            temp=(test_input[:,i,:,:]/test_input[:,i,:,:].max()).reshape(test_input.shape[0],
                                                                                     1, test_input.shape[2],
                                                                                         test_input.shape[3])
            test_input2=torch.cat((test_input2,temp), dim=1)
    return trainset_input2, valid_input2, test_input2
-------------------------------------------------------------------------------------------



------------------------------Created input matrices with script:------------------------------
import numpy as np
import torch


def parse_matrices(file, many_classes, classes):
    bases={'A': 0, 'C': 1, 'G': 2, 'T': 3}
    #classes={'AA': 0, 'AC': 1, 'AG': 2, 'AT': 3, 'CA': 4, 'CC': 5, 'CG': 6, 'CT': 7, 'GA':8, 'GC': 9, 'GG': 10,
    #        'GT': 11, 'TA': 12, 'TC': 13, 'TG': 14, 'TT': 15}
    #classes={'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N':4}
    #classes={'altA': 0, 'altC': 1, 'altG': 2, 'altT': 3, 'refA': 4, 'refC': 5, 'refG': 6, 'refT': 7}
    row=0
    #temp_3d=np.empty((1,1,1))
    df_target=np.empty((1,1,1))
    first_2d=True
    first_3d=True
    first_4d_input=True
    first_df_input=True
    new_sampleid=True
    previous_id=None
    single_class=list()
    target=list()
    row_list=list()
    with open(file, 'r') as fr:
        for line in fr:
            if line.startswith('#'):
                if line.startswith("#ID:"):
                    if previous_id==None:
                        previous_id=line.strip()
                        continue
                    if first_4d_input:
                        temp_4d=temp_3d
                        first_4d_input=False
                    else:
                        temp_4d=np.concatenate((temp_4d, temp_3d), axis=1)
                    first_3d=True
                    if line.strip()!=previous_id:
                        new_sampleid=True
                        if first_df_input:
                            df_input=temp_4d
                            first_df_input=False
                        else:
                            df_input=np.concatenate((df_input, temp_4d), axis=0)
                        first_4d_input=True
                        previous_id=line.strip()
                if many_classes:
                    if line.startswith('#REF:'):
                        ref=line[6]
                    elif line.startswith('#ALT:') and new_sampleid:
                        alt=line[6]
                        target.append(torch.tensor(classes[ref+alt]))
                        new_sampleid=False
                else:
                    if "CLASS: random" in line and new_sampleid:
                        new_sampleid=False
                        if many_classes:
                            target.append(torch.tensor(0))
                        else:
                            type="neg"
                    elif ("#CLASS: train" in line) or ("#CLASS: predict" in line) and new_sampleid:
                        new_sampleid=False
                        if many_classes:
                            target.append(torch.tensor(1))
                        else:
                            type="pos"
                continue
            numbers = [float(number) for number in line.strip().split(',')]
            row_in_array=np.array(numbers).reshape(1,1,1,-1)
            if first_2d:
                temp_2d=row_in_array
                first_2d=False
            else:
                temp_2d=np.concatenate((temp_2d, row_in_array), axis=2)
            row+=1
            if row==4:
                if first_3d:
                    temp_3d=temp_2d
                    first_3d=False
                else:
                    temp_3d=np.concatenate((temp_3d, temp_2d), axis=1)
                first_2d=True
                row=0
    if not first_3d:
        if first_4d_input:
            temp_4d=temp_3d
        else:
            temp_4d=np.concatenate((temp_4d, temp_3d), axis=1)
        df_input=np.concatenate((df_input, temp_4d), axis=0)
    if many_classes:
        return torch.Tensor(df_input), torch.tensor(target,dtype=torch.long)
    else:
        return torch.Tensor(df_input), torch.tensor(target,dtype=torch.float)
    
    
if __name__=='__main__':
    raise RuntimeError()
-------------------------------------------------------------------------------------------



------------------------------Computed accuracies with script:-----------------------------
import torch

def count_f_scores(tp,fp,tn,fn):
    if (tp+fp)==0:
        precision=0
    else:
        precision=tp/(tp+fp)
    if tp+fn==0:
        recall=0
    else:
        recall=tp/(tp+fn)
    if precision==0 and recall==0:
        f1=f2=0
    else:
        f1=2*(precision*recall)/(precision+recall)
        f2=5*(precision*recall)/(4*precision+recall)
    return f1, f2, precision, recall

def count_pred_types(ftp, ftn, tp,fp,tn,fn, labels, predicted, sequences):
    for label, prediction, sequence in zip(labels, predicted, sequences):
        #if label in [0,5,10,15]:
        #if label==4:
        if label in [1,7]:
        #if label in [4,5,6,7]:
            if label==prediction:
                tn+=1
            else:
                fp+=1
                #if prediction in [0,5,10,15]:
                if prediction in [1,7]:
                #if prediction in [4,5,6,7]:
                #if prediction==4:
                    ftn+=1
        else:
            if label==prediction:
                tp+=1
            else:
                fn+=1
                #if prediction!=4:
                #if prediction not in [0,5,10,15]:
                if prediction not in [1,7]:
                #if not prediction in [4,5,6,7]:
                    #for i in range(4):
                    #    if sequence[0, i,16]==1:
                    #        ref=i
                    #if prediction!=i:
                     #   ftn+=1
                    ftp+=1
    return tp,fp,tn,fn,ftn, ftp

def compute_accuracy(device, net, dataloader, criterion, many_classes, datatype, verbose):
    net.eval()
    correct = 0
    total = 0
    tp=fp=tn=fn=ftn=ftp=0
    number_of_classes=8
    correct_separated=[0 for i in range(number_of_classes)]
    correct_totals=[0 for i in range(number_of_classes)]
    with torch.no_grad():
        tot_loss=0
        tot_items=0
        for sequences, labels in dataloader:
            sequences, labels = sequences.to(device), labels.to(device)
            outputs = net(sequences)
            if not many_classes:
                outputs=outputs.squeeze()
                predicted = torch.round(outputs.data)
            else:
                _, predicted = torch.max(outputs.data, 1)
            for label, prediction in zip(labels, predicted):
                correct_separated[label]+=(label==prediction).item()
                correct_totals[label]+=1
            tot_loss+=criterion(outputs,labels).item()
            tot_items+=len(labels)
            #total += labels.size(0)
            #correct += (predicted == labels).sum().item()
            tp,fp,tn,fn, ftn, ftp = count_pred_types(ftp, ftn, tp,fp,tn,fn, labels, predicted, sequences)
        tot_loss/=tot_items
        for i in range(number_of_classes):
            correct+=(correct_separated[i]/correct_totals[i])/number_of_classes
        f1, f2, precision, recall = count_f_scores(tp,fp,tn,fn)
        #f1_fake, f2_fake = count_f_scores((tp+ftn),fp,tn,(fn-ftn))
        f1_fake, f2_fake, fake_precision, fake_recall = count_f_scores((tp+ftp),(fp-ftn),(tn+ftn),(fn-ftp))
        if verbose:
            print('\n',datatype)
            print("TP:",tp,". FN:",fn, "TP/(TP+FN):",tp/(tp+fn),"TN:",tn,"FP:",fp,"TN/(TN+FP):",tn/(tn+fp),
                  "Wrong positive class predicted:",ftp, "Wrong negative class predicted:",ftn)
            print("Fake F1-score:",f1_fake,". Fake F2-score:",f2_fake)
            print("Fake TP/(TP+FN):",(tp+ftp)/(tp+ftp+fn),"Fake TN/(TN+FP)",(tn+ftn)/(tn+ftn+fp))
            print("Fake precision:",fake_precision,"Fake recall:",fake_recall)
    #return correct / total, tot_loss, f1, f2
    return correct, tot_loss, f1, f2, precision, recall, f1_fake, f2_fake, fake_precision, fake_recall
-------------------------------------------------------------------------------------------
