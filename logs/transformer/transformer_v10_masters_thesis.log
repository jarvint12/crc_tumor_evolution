Created: 08.01.2024 13:07:22
Model version: transformer_v10
Path: saved_models/transformer/transformer_v10.pth
Run name: None
Accuracy: 0.4994920418074643

Hyperparameters:
Optimizer: AdamW
Learning rate: 0.0001
Weight decay: 1e-06
head: 42
num_encoder_layers: 6
num_decoder_layers: 6
Used MSELoss
Balanced classes
Data normalized: True
Norm type: None
Target mode: whole_matrix
Train batch size: 32
Validation batch size: 5


--------------------------Script of the model can be seen below.---------------------------
import torch
import torch.nn as nn
import os
import math

current_file_name = os.path.basename(__file__)
model_version=current_file_name.split('.')[0]

class WeightedMSELoss(nn.Module):
    def __init__(self, device, class_weights, classes, correct_label_index, indices_of_interest):
        super(WeightedMSELoss, self).__init__()
        self.weights=class_weights
        self.classes=classes
        self.num_classes=len(self.classes)
        self.correct_label_index=correct_label_index
        self.indices_of_interest = indices_of_interest
        self.bases= {0: 'A', 1: 'C', 2: 'G', 3: 'T'}
        self.device=device
            
    def forward(self, sequences, targets, outputs):
        values_of_interest = sequences[:, 16, self.indices_of_interest]
        _, original_bases = torch.max(values_of_interest, dim=1)
        
        values_of_interest = targets[:, self.correct_label_index, self.indices_of_interest]
        _, new_bases = torch.max(values_of_interest, dim=1)
        correct_classes = [self.classes[self.bases[original.item()] + self.bases[new_.item()]] for original, new_ in zip(original_bases, new_bases)]

        # Calculate weights based on correct classes
        weights = torch.tensor([self.weights[class_] for class_ in correct_classes]).to(self.device)
        weights = weights.view(-1, 1, 1).expand_as(outputs)
        # Calculate MSE loss
        loss = 100*torch.mean((outputs - targets) ** 2 * weights)
        return loss
    
    
class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, max_len):
        super(PositionalEncoding, self).__init__()

        # Calculate positional encodings
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, dim_model, 2).float() * -(math.log(10000.0) / dim_model))
        pos_encoding = torch.zeros((max_len, dim_model))
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        pos_encoding = pos_encoding.unsqueeze(0)

        self.register_buffer('pos_encoding', pos_encoding)

    def forward(self, token_embedding):
        # Expand the positional encoding to match the batch size
        pos_encoding = self.pos_encoding[:, :token_embedding.size(1)].expand(token_embedding.size(0), -1, -1)
        
        # Residual connection + pos encoding
        return token_embedding + pos_encoding
    
    
    
class Transformer(nn.Module):
    def __init__(self, d_model=84, nhead=8, num_encoder_layers=6, num_decoder_layers=6):
        super(Transformer, self).__init__()
        self.positional_encoder = PositionalEncoding(
            dim_model=d_model, max_len=34
        )
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,
                                          num_encoder_layers=num_encoder_layers,
                                          num_decoder_layers=num_decoder_layers,
                                         batch_first=True)
        self.out = nn.Linear(d_model, d_model)

    def forward(self, src, tgt, tgt_mask=None):
        """
        Parameters:
        src: Input matrix of shape (batch_size, seq_len, embedding_dim),
                seq_len=33, embedding_dim=84
        tgt: Target matrix of shape (batch_size, seq_len, embedding_dim)"""
        src = self.positional_encoder(src)
        tgt = self.positional_encoder(tgt) #
        output = self.transformer(src, tgt, tgt_mask=tgt_mask)
        output = self.out(output)
        return output
    
    
    def get_tgt_mask(self, size, device) -> torch.tensor:
        # Generates a squeare matrix where the each row allows one word more to be seen
        mask = torch.tril(torch.ones((size, size), device=device) == 1) # Lower triangular matrix
        mask = mask.float()
        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf
        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0
        #mask = torch.triu(torch.ones((size, size), device=device, dtype=torch.bool), diagonal=1)

        return mask
-------------------------------------------------------------------------------------------



------------------------------Created input matrices with script:------------------------------
import numpy as np
import torch
from torch.utils.data import Dataset
import collections

class FeatureDataset(Dataset):
    def __init__(self, data, labels):
        '''
        Args:
        info_dir (string/pandas Dataframe): Path to excel file(or the file itself), that contains clinical info about PET images
        seed (int): seed for sampling images
        norm_mean_std (str): string that indicates the means and stds for normalization
        prob_gaussian (float): Probability for blurring images
        simple_transformation (bool): Whether to use minimal transformations
        
        Outputs:
        image (torch.Tensor): Image as torch Tensor. Shape (1,3,512,512)
        label (torch.Tensor): Label indicating if there is cancer in the picture. 1=Cancer, 0=Benign 
        '''
        self.data_tensor=data
        self.labels=labels

    def __len__(self):
        return len(self.data_tensor)
    
    def __getitem__(self, idx):
        #load images
        tensor = self.data_tensor[idx]
        #load labels 
        label = self.labels[idx]
        
        return tensor, label


def create_class_weights(class_amounts, total_amount, norm_type):
    class_weights=collections.Counter()
    for target_class in class_amounts:
        weight = total_amount / (len(class_amounts) * class_amounts[target_class]) #tot_samples/(n_classes*n_samples_in_class)
        class_weights[target_class] = weight
    if norm_type=="sum":
        # normalize by sum
        sum_weights = sum(class_weights.values())
        for target_class in class_weights:
            class_weights[target_class] = class_weights[target_class] / sum_weights
    elif norm_type=="max":
        # normalize by max
        max_weight = max(class_weights.values())
        for target_class in class_weights:
            class_weights[target_class] = class_weights[target_class] / max_weight
    return class_weights

    
def get_norm_values(file):
    numbers=dict()
    previous_id=None
    with open(file, 'r') as fr:
        for line in fr:
            if line.startswith('#'):
                if line.startswith("#ID:"):
                        if line.strip()!=previous_id:
                            previous_id=line.strip()
                            channel=0
                        else:
                            channel+=1
                continue
            if not channel in numbers:
                numbers[channel]=list()
            numbers[channel] += [float(number) for number in line.strip().split(',')]
    mins=dict()
    maxes=dict()
    for channel in numbers:
        mins[channel]=min(numbers[channel])
        maxes[channel]=max(numbers[channel])
    return mins, maxes


def create_target(target_mode, temp_2d, target_row, first_genome_row):
    temp_target = np.copy(temp_2d) #(1, 33, 84)
    for i in range(first_genome_row, first_genome_row+4):
        temp_target[0, 16, i]=int(i==first_genome_row+target_row) #Change target base to 1 and every other to 0
    if target_mode not in ["whole_matrix", "target_with_landscape"]:
        
        for i in range(temp_target.shape[2]): #Go through every row in matrix
            if first_genome_row<=i<first_genome_row+4: #Skip DNA sequence rows
                continue
            for j in range(33): #Go through every col in row
                temp_target[0, j, i] = 0 #Change all values to 0, so only DNA sequences are chosen
    if target_mode in ["only_target_base", "target_with_landscape"]:
        temp_target=temp_target[0,16,:].reshape(1,1,-1)
    return temp_target

def parse_matrices(file, norm, target_mode, norm_type):
    assert target_mode in ["whole_matrix", "whole_DNA_seq", "only_target_base", "target_with_landscape"], \
                            "Incorrect target mode {}".format(target_mode)
    first_genome_row=80
    if norm:
        mins, maxes = get_norm_values(file)
    bases={'A': 0, 'C': 1, 'G': 2, 'T': 3}
    classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7} #For class weights
    class_amounts=collections.Counter() #For class weights
    total_amount=0 #For class weights
    first_row_of_sample=True
    first_sample=True
    df_input=None
    new_sampleid=True
    previous_id=None
    with open(file, 'r') as fr:
        for line in fr:
            if line.startswith('#'):
                if line.startswith("#ID:"):
                    if previous_id==None:
                        previous_id=line.strip()
                        channel=0
                        continue
                    elif line.strip()!=previous_id:
                        new_sampleid=True
                        channel=0
                        temp_target = create_target(target_mode, temp_2d, target_row, first_genome_row)
                        
                        if first_sample:
                            df_input=temp_2d
                            df_target=temp_target
                            first_sample=False
                        else:
                            df_input=np.concatenate((df_input, temp_2d), axis=0)
                            df_target=np.concatenate((df_target, temp_target), axis=0) #(batch_size, 33, 84)
                        first_row_of_sample=True
                        previous_id=line.strip()
                    else:
                        channel+=1
                if new_sampleid and line.startswith('#ALT:'):
                        alt=line[6]
                        target_row = bases[alt]
                        new_sampleid=False
                        class_amounts[classes[ref+alt]]+=1
                        total_amount+=1
                elif line.startswith('#REF:'):
                        ref=line[6]
                continue
            if norm:
                numbers = [(float(number)-mins[channel])/(maxes[channel]-mins[channel]) for number in line.strip().split(',')]
            else:
                numbers= [float(number) for number in line.strip().split(',')]
            row_in_array=np.array(numbers).reshape(1,-1,1)
            if first_row_of_sample:
                temp_2d=row_in_array
                first_row_of_sample=False
            else:
                temp_2d=np.concatenate((temp_2d, row_in_array), axis=2)

    if not first_row_of_sample:
        df_input=np.concatenate((df_input, temp_2d), axis=0)
        temp_target = create_target(target_mode, temp_2d, target_row, first_genome_row)
        df_target=np.concatenate((df_target, temp_target), axis=0)
        
    class_weights = create_class_weights(class_amounts, total_amount, norm_type=norm_type)
    return torch.Tensor(df_input), torch.Tensor(df_target), class_weights
    
    
if __name__=='__main__':
    raise RuntimeError()
-------------------------------------------------------------------------------------------



------------------------------Computed accuracies with script:-----------------------------
import torch

def count_f_scores(tp,fp,tn,fn):
    if (tp+fp)==0:
        precision=0
    else:
        precision=tp/(tp+fp)
    if tp+fn==0:
        recall=0
    else:
        recall=tp/(tp+fn)
    if precision==0 and recall==0:
        f1=f2=0
    else:
        f1=2*(precision*recall)/(precision+recall)
        f2=5*(precision*recall)/(4*precision+recall)
    return f1, f2, precision, recall

def count_pred_types(ftp, ftn, tp,fp,tn,fn, labels, predicted, original_bases):
    for original_base, label, prediction in zip(original_bases, labels, predicted):
        if original_base==label:
            if label==prediction:
                tn+=1
            else:
                fp+=1
                if prediction in [1,7]:
                    ftn+=1
        else:
            if label==prediction:
                tp+=1
            else:
                fn+=1
                if prediction not in [1,7]:
                    ftp+=1
    return tp,fp,tn,fn,ftn, ftp

def compute_accuracy(device, net, dataloader, criterion, datatype, verbose, cv, correct_label_index):
    net.eval()
    bases = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}
    classes={'CA': 0, 'CC': 1, 'CG': 2, 'CT': 3, 'TA': 4, 'TC': 5, 'TG': 6, 'TT': 7}
    correct = 0
    tp=fp=tn=fn=ftn=ftp=0
    number_of_classes=len(classes)
    correct_per_class=[0 for i in range(number_of_classes)]
    total_per_class=[0 for i in range(number_of_classes)]
    with torch.no_grad():
        tot_loss=0
        tot_items=0
        for sequences, labels in dataloader:
            sequences, labels = sequences.to(device), labels.to(device)
            
            labels_input = labels[:,:-1]
            labels_expected = labels[:,1:]
                        
            sequence_length = labels_input.size(1)
            tgt_mask = net.get_tgt_mask(sequence_length, device)
                        
            output = net(sequences, labels_input, tgt_mask=tgt_mask)
            indices_of_interest = [80, 81, 82, 83]
            
            #Get the predicted bases, output should not contain SOS token? Thus, the correct base in 16th/0th position
            values_of_interest = output[:, correct_label_index, indices_of_interest] #Start token removed
            max_values, results = torch.max(values_of_interest, dim=1)
            
            #Get the expected bases, shifted right, so does not contain SOS token. Thus, the correct base in 16th/0th position
            values_of_interest = labels_expected[:, correct_label_index, indices_of_interest] #
            correct_max_values, correct_results = torch.max(values_of_interest, dim=1)
            
            #Get the input base. Input does not contain SOS token, and is always 33 bases long. Thus, the correct base in 16th position
            values_of_interest = sequences[:, 16, indices_of_interest]
            _, original_bases = torch.max(values_of_interest, dim=1)
            
            for result, correct_result, original_base in zip(results, correct_results, original_bases):
                correct_class = classes[bases[original_base.item()]+bases[correct_result.item()]]
                correct_per_class[correct_class]+=(correct_result==result).item()
                total_per_class[correct_class]+=1

            if cv or verbose:
                tot_items+=len(labels)
            if cv:
                tot_loss+=criterion(sequences, labels_expected, output).item()
            if verbose:
                tp,fp,tn,fn, ftn, ftp = count_pred_types(ftp, ftn, tp,fp,tn,fn, correct_results, results, original_bases)
        if cv:
            tot_loss/=tot_items
        for i in range(number_of_classes):
            correct+=(correct_per_class[i]/total_per_class[i])/number_of_classes
        if verbose:
            f1, f2, precision, recall = count_f_scores(tp,fp,tn,fn)
            f1_fake, f2_fake, fake_precision, fake_recall = count_f_scores((tp+ftp),(fp-ftn),(tn+ftn),(fn-ftp))
            tn_tnfp=tn/(tn+fp) if tn+fp>0 else 0
            fake_tpftp_tpftpfn=(tp+ftp)/(tp+ftp+fn) if (tp+ftp+fn)>0 else 0
            fake_tnftn_tnftnfp=(tn+ftn)/(tn+ftn+fp) if (tn+ftn+fp)>0 else 0
            print('\n',datatype)
            print("TP:",tp,". FN:",fn, "TP/(TP+FN):",recall,"TN:",tn,"FP:",fp,"TN/(TN+FP):",tn_tnfp,
                  "Wrong positive class predicted:",ftp, "Wrong negative class predicted:",ftn)
            print("Fake F1-score:",f1_fake,". Fake F2-score:",f2_fake)
            print("Fake TP/(TP+FN):",fake_tpftp_tpftpfn,"Fake TN/(TN+FP)",fake_tnftn_tnftnfp)
            print("Fake precision:",fake_precision,"Fake recall:",fake_recall)
            print("F1-score:",f1)
            print("F2-score:",f2)
            print("Precision:",precision)
            print("Recall:",recall)
            print("Fake accuracy:",(tn+ftn+tp+ftp)/tot_items)

    return correct, tot_loss
-------------------------------------------------------------------------------------------
